% !TEX root = main.tex
\clearpage

\section{Basics of diffusion models}\label{sec:diffusion_basics}

\begin{notebox}
\textbf{Using:} 
\fullcite{ho_denoising_2020} \\
\fullcite{weng_what_2021}.
\end{notebox}

\subsection{Reverse process}

We have true data coming from an unknown underlying distribution $\rvx_0 \sim q(\rvx_0)$.

We assume a latent variable model $\pt(\rvx_0) \approx q(\rvx_0)$ approximating the true distribution $\pt(\rvx_0) = \int \pt(\rvx_0 | \rvx_{1:T}) \pt(\rvx_{1:T}) \, d\rvx_{1:T}$.
For this we assume a learned \addidx{prior} $\pt(\rvx_{1:T})$ with Markov chain with Gaussian transitions (the means and variances are learned):
\begin{align*}
    \pt(\rvx_{1:T}) &= p(\rvx_T) \prod_{t=2}^T \pt(\rvx_{t-1} | \rvx_t) \\
    \pt(\rvx_{t-1} | \rvx_t) &= \dN(\rvx_{t-1}; \mt(\rvx_t, t), \st(\rvx_t, t)) \\
    p(\rvx_T) & = \dN(\rvx_T; \mathbf{0}, \mI) \enspace .
\end{align*}
The complete joint distribution $\pt(\rvx_{0:T})$ is called the \addidx{reverse process}.

\subsection{Variational bound}

\paragraph{Follow the logic of importance sampling of VAE (see technical notes 2019):}
We could start maximizing the likelihood $\pt(\rvx_0)$ directly from 
\begin{equation}
    \pt(\rvx_0) = \int \pt(\rvx_{1:T}) \pt(\rvx_0 | \rvx_{1:T}) \, d\rvx_{1:T}
\end{equation}
by sampling from the prior $\pt(\rvx_{1:T})$.
Same as always, this would take very long cause the prior samples won't be very informative for the true data and won't give enough information for the training.

We could instead sample from the posterior $\pt(\rvx_{1:T} | \rvx_0) = \frac{\pt(\rvx_{0:T})}{\pt(\rvx_0)}$ using the \addidx{importance sampling} which should be more informative
\begin{equation}
    \pt(\rvx_0) = \int \pt(\rvx_{1:T} | \rvx_0) \frac{\pt(\rvx_{1:T}) \pt(\rvx_0 | \rvx_{1:T})}{\pt(\rvx_{1:T} | \rvx_0)} \, d\rvx_{1:T}
\end{equation}
The problem as always is that the posterior is intractable due to the unknown evidence $\pt(\rvx_0)$.

Hence we need an approximation instead $q(\rvx_{1:T} | \rvx_0) \approx \pt(\rvx_{1:T} | \rvx_0)$ so that
\begin{equation}
    \pt(\rvx_0) = \int q(\rvx_{1:T} | \rvx_0) \frac{\pt(\rvx_{1:T}) \pt(\rvx_0 | \rvx_{1:T})}{q(\rvx_{1:T} | \rvx_0)} \, d\rvx_{1:T}
\end{equation}

This indeed is very similar to a VAE when we indicate the whole sequence $\rvx_{1:T}$ as a single latent variable $\rvz$.

We can now maximize the model likelihood $\pt(\rvx_0)$ by sampling the latent variable $\rvx_{1:T} \sim q(\rvx_{1:T} | \rvx_0)$ from the approximate posteriors which can be seen as the \addidx{encoder}.
We also have the learned \addidx{decoder} $\pt(\rvx_0 | \rvx_{1:T})$ and a prior $\pt(\rvx_{1:T})$ which in this case is learned.

We could train the model from the classical variational bound on the log likelihood
\begin{equation}
    \log \pt(\rvx_0) 
    = 
    \log \int q(\rvx_{1:T} | \rvx_0) \frac{\pt(\rvx_{0:T})}{q(\rvx_{1:T} | \rvx_0)} \, d\rvx_{1:T}
    \geq 
    \int q(\rvx_{1:T} | \rvx_0) \log \frac{\pt(\rvx_{0:T})}{q(\rvx_{1:T} | \rvx_0)} \, d\rvx_{1:T}
\end{equation}

\paragraph{VAE-like minimization problem:} 
Classically we minimize the negative log likelihood.
The objective is thus the minimization of the variational bound
\begin{align}\label{eq:VAE_loss}
    \Ls(\rvx_0) & = \E_{q(\rvx_{1:T} | \rvx_0)} - \log \frac{\pt(\rvx_{0:T})}{q(\rvx_{1:T} | \rvx_0)} \nn
     & = \E_{q(\rvx_{1:T} | \rvx_0)} \log \frac{q(\rvx_{1:T} | \rvx_0)}{\pt(\rvx_{0:T-1}|\rvx_T) \pt(\rvx_T)} \nn
     & = \E_{q(\rvx_{1:T} | \rvx_0)} \Big[- \log \pt(\rvx_T) - \log \prod_{t=1}^T \pt(\rvx_{t-1}|\rvx_{t}) +  \log q(\rvx_{1:T} | \rvx_0) \Big] 
\end{align}

\subsection{Forward process}

For the approximate posterior we assume again a Markov chain but now in the other direction as a \addidx{forward process} and with known Gaussian transitions with a variance schedule $\beta_1, \ldots, \beta_T$
\begin{align*}
    q(\rvx_{1:T} | \rvx_0) &= \prod_{t=1}^T q(\rvx_{t} | \rvx_{t-1}) \\
    q(\rvx_{t} | \rvx_{t-1}) &= \dN(\rvx_{t}; \sqrt{1-\beta_t} \rvx_{t-1}, \beta_t \mI) \enspace .
\end{align*}

\paragraph{VAE-like minimization with forward process:} 
The minimization problem \eqref{eq:VAE_loss} can now be written as
\begin{align}\label{eq:VAE_fwd}
    \Ls(\rvx_0) &= \E_{q(\rvx_{1:T} | \rvx_0)} \log \frac{\prod_{t=1}^T q(\rvx_{t} | \rvx_{t-1})}{p(\rvx_T) \prod_{t=1}^T \pt(\rvx_{t-1} | \rvx_t) } \nn
    &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \log \prod_{t=1}^T \frac{q(\rvx_{t} | \rvx_{t-1})}{\pt(\rvx_{t-1} | \rvx_t) } \right] \nn
    &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \sum_{t=1}^T \log \frac{q(\rvx_{t} | \rvx_{t-1})}{\pt(\rvx_{t-1} | \rvx_t) } \right]
\end{align}


\paragraph{Sampling from forward process:}
We can sample from forward process $q(\rvx_{1:T} | \rvx_0)$ recursively through $\rvx_t \sim \dN(\sqrt{1-\beta_t)} \rvx_{t-1}, \beta_t \mI)$ by fixing the previous value and sampling a noise $\rvepsilon \sim \dN(\mathbf{0}, \mI)$
\begin{equation}
\rvx_t = \sqrt{1-\beta_t} \rvx_{t-1} + \sqrt{\beta_t} \rvepsilon \quad 
\E(\rvx_t) = \sqrt{1-\beta_t} \rvx_{t-1} \quad
\Var(\rvx_t) = \beta_t \mI
\end{equation}
Let us indicate $\alpha_t = 1-\beta_t$ and hence $\beta_t = 1-\alpha_t$

We then get
\begin{align*}
    \rvx_1 &\sim \dN(\sqrt{\alpha_1} \rvx_0, (1 -\alpha_1) \mI) \\
    \rvx_1 &= \sqrt{\alpha_1} \rvx_0 + \sqrt{1 -\alpha_1} \rvepsilon_0 \\
\end{align*}
\begin{align*}
    \rvx_2 &\sim \dN(\sqrt{\alpha_2} \rvx_1, (1 -\alpha_2) \mI) \\
    \rvx_2 &= \sqrt{\alpha_2} \rvx_1 + \sqrt{1 -\alpha_2} \rvepsilon_1 \\
    &= \sqrt{\alpha_2}(\sqrt{\alpha_1} \rvx_0 + \sqrt{1 -\alpha_1} \rvepsilon_0) + \sqrt{1 -\alpha_2} \rvepsilon_1\\
    &= \sqrt{\alpha_1\alpha_2} \rvx_0 + \sqrt{\alpha_2 -\alpha_1\alpha_2} \rvepsilon_0 + \sqrt{1 -\alpha_2} \rvepsilon_1 \\
    \E(\rvx_2) &= \sqrt{\alpha_1\alpha_2} \rvx_0 \\
    \Var(\rvx_2) &= (\alpha_2 -\alpha_1\alpha_2 + 1 - \alpha_2) \mI = (1 - \alpha_1\alpha_2) \mI \\
    \rvx_2 &= \sqrt{\alpha_1\alpha_2} \rvx_0 + \sqrt{1 - \alpha_1\alpha_2} \rvepsilon \\
    \rvx_2 &\sim \dN(\sqrt{\alpha_1\alpha_2} \rvx_0, (1 - \alpha_1\alpha_2) \mI)
\end{align*}
\begin{align*}
    \rvx_3 &\sim \dN(\sqrt{\alpha_3} \rvx_2, (1 -\alpha_3) \mI) \\
    \rvx_2 &= \sqrt{\alpha_3} \rvx_2 + \sqrt{1 -\alpha_3} \rvepsilon_2 \\
    &= \sqrt{\alpha_3}(\sqrt{\alpha_1\alpha_2} \rvx_0 + \sqrt{\alpha_2 -\alpha_1\alpha_2} \rvepsilon_0 + \sqrt{1 -\alpha_2} \rvepsilon_1) + \sqrt{1 -\alpha_3} \rvepsilon_2\\
    &= \sqrt{\alpha_1\alpha_2\alpha_3} \rvx_0 + \sqrt{\alpha_2\alpha_3 -\alpha_1\alpha_2\alpha_3} \rvepsilon_0 + \sqrt{\alpha_3 -\alpha_2\alpha_3} \rvepsilon_1 + \sqrt{1 -\alpha_3} \rvepsilon_2\\
    \E(\rvx_2) &= \sqrt{\alpha_1\alpha_2\alpha_3} \rvx_0 \\
    \Var(\rvx_2) &= (\alpha_2\alpha_3 -\alpha_1\alpha_2\alpha_3 + \alpha_3 -\alpha_2\alpha_3 + 1 -\alpha_3) \mI = (1 - \alpha_1\alpha_2\alpha_3) \mI \\
    \rvx_2 &= \sqrt{\alpha_1\alpha_2\alpha_3} \rvx_0 + \sqrt{1 - \alpha_1\alpha_2\alpha_3} \rvepsilon \\
    \rvx_2 &\sim \dN(\sqrt{\alpha_1\alpha_2\alpha_3} \rvx_0, (1 - \alpha_1\alpha_2\alpha_3) \mI)
\end{align*}
and in general
\begin{align}\label{eq:xt_sampling}
    \rvx_t &\sim \dN(\sqrt{\alpha_t} \rvx_{t-1}, (1 -\alpha_t) \mI) \nn
    \rvx_t &= \sqrt{\alpha_t} \rvx_{t-1} + \sqrt{1 -\alpha_t} \rvepsilon_{t-1} \nn
    \rvx_t &= \sqrt{\prod_{s=1}^t \alpha_s} \rvx_0 + \sqrt{1 - \prod_{s=1}^t \alpha_s} \rvepsilon \nn
    \rvx_t &= \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon \\
    \rvx_t &\sim \dN(\sqrt{\bar{\alpha}_t} \rvx_0, (1 - \bar{\alpha}_t) \mI) = q(\rvx_t | \rvx_0) \nonumber
\end{align}

In summary, instead of fixing the variance schedule $\beta_1 \ldots \beta_T$ and sampling from the forward process recursively 
\begin{equation}
    \rvx_t \sim \dN(\sqrt{1-\beta_t} \rvx_{t-1}, \beta_t \mI) = q(\rvx_t | \rvx_{t-1}) \quad \text{via} \quad \rvx_t = \sqrt{1-\beta_t} \rvx_{t-1} + \sqrt{\beta_t} \rvepsilon, \quad \rvepsilon \sim \dN(\mathbf{0}, \mI)
\end{equation}
we can fix the schedule $\bar{\alpha}_1 \ldots \bar{\alpha}_T$ and sample arbitrary timestep directly from
\begin{equation}
    \rvx_t \sim \dN(\sqrt{\bar{\alpha}_t} \rvx_0, (1 - \bar{\alpha}_t) \mI)  = q(\rvx_t | \rvx_0) \quad \text{via} \quad \rvx_t = \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon, \quad \rvepsilon \sim \dN(\mathbf{0}, \mI)
\end{equation}

\paragraph{Minimization with $x_0$ conditioning:}
We can further play with the variational bound \eqref{eq:VAE_fwd}
\begin{align}
    \Ls(\rvx_0) &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \sum_{t=1}^T \log \frac{q(\rvx_{t} | \rvx_{t-1})}{\pt(\rvx_{t-1} | \rvx_t) } \right] \nn
    &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \sum_{t=2}^T \log \frac{q(\rvx_{t} | \rvx_{t-1})}{\pt(\rvx_{t-1} | \rvx_t) } + \log \frac{q(\rvx_1 | \rvx_0)}{\pt(\rvx_0 | \rvx_1) } \right] \nn
\end{align} 

Now we use the following for the forward process
\begin{align}
    q(\rvx_t, \rvx_{t-1}, \rvx_0) & = q(\rvx_t | \rvx_{t-1}, \rvx_0) q(\rvx_{t-1} | \rvx_0) q(\rvx_0) \nn
    & = q(\rvx_t | \rvx_{t-1}) q(\rvx_{t-1} | \rvx_0) q(\rvx_0) \quad \text{(Markov assumption on forward process)} \nn
    q(\rvx_t, \rvx_{t-1}, \rvx_0) & = q(\rvx_{t-1} | \rvx_t, \rvx_0) q(\rvx_t | \rvx_0) q(\rvx_0)
\end{align}
so that
\begin{align}
    q(\rvx_t | \rvx_{t-1}) &= q(\rvx_t | \rvx_{t-1}, \rvx_0) \nn
    &= \frac{q(\rvx_t, \rvx_{t-1}, \rvx_0)}{q(\rvx_{t-1} | \rvx_0) q(\rvx_0)} \nn
    &= \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0) q(\rvx_t | \rvx_0) q(\rvx_0)}{q(\rvx_{t-1} | \rvx_0) q(\rvx_0)}
\end{align}
to rewrite the minimization as
\begin{align}
    \Ls(\rvx_0) &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \sum_{t=2}^T \log \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0)}{\pt(\rvx_{t-1} | \rvx_t) } + \sum_{t=2}^T \log \frac{q(\rvx_t | \rvx_0)}{q(\rvx_{t-1} | \rvx_0)} + \log \frac{q(\rvx_1 | \rvx_0)}{\pt(\rvx_0 | \rvx_1) } \right] \nn
\end{align} 
We then observe that
\begin{align}
    \sum_{t=2}^T \log \frac{q(\rvx_t | \rvx_0)}{q(\rvx_{t-1} | \rvx_0)} &= \log \prod_{t=2}^T \frac{q(\rvx_t | \rvx_0)}{q(\rvx_{t-1} | \rvx_0)} \nn
    &= \log \frac{q(\rvx_2 | \rvx_0)}{q(\rvx_1 | \rvx_0)}\frac{q(\rvx_3 | \rvx_0)}{q(\rvx_2 | \rvx_0)}\frac{q(\rvx_4 | \rvx_0)}{q(\rvx_3 | \rvx_0)} \ldots \frac{q(\rvx_T | \rvx_0)}{q(\rvx_T-1 | \rvx_0)} \nn
    &= \log \frac{q(\rvx_T | \rvx_0)}{q(\rvx_1 | \rvx_0)}
\end{align}
and hence get for a single sample

\begin{align}\label{eq:ELBOSingleVar}
    \Ls(\rvx_0)
    &= \E_{q(\rvx_{1:T} | \rvx_0)} \left[ - \log p(\rvx_T) + \sum_{t=2}^T \log \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0)}{\pt(\rvx_{t-1} | \rvx_t) } + \log \frac{q(\rvx_T | \rvx_0)}{q(\rvx_1 | \rvx_0)} + \log \frac{q(\rvx_1 | \rvx_0)}{\pt(\rvx_0 | \rvx_1) } \right] \nn
    &= \E_{q(\rvx_{T}  | \rvx_0)} \log \frac{q(\rvx_T | \rvx_0)}{p(\rvx_T)}
    - \E_{q(\rvx_1  | \rvx_0)} \log \pt(\rvx_0 | \rvx_1)
    + \E_{q(\rvx_{1:T} | \rvx_0)} \sum_{t=2}^T \log \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0)}{\pt(\rvx_{t-1} | \rvx_t) } 
\end{align}


and in expectation for the complete data set
\begin{align}\label{eq:VarBoundLoss}
    \Ls &= \E_{q(\rvx_0)} \Ls(\rvx_0) \nn
    &= \E_{q(\rvx_{0:T})} \left[ - \log p(\rvx_T) + \sum_{t=2}^T \log \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0)}{\pt(\rvx_{t-1} | \rvx_t) } + \log \frac{q(\rvx_T | \rvx_0)}{q(\rvx_1 | \rvx_0)} + \log \frac{q(\rvx_1 | \rvx_0)}{\pt(\rvx_0 | \rvx_1) } \right] \nn
    &= \E_{q(\rvx_{0:T})} \left[ - \log \frac{p(\rvx_T)}{q(\rvx_T | \rvx_0)} + \sum_{t=2}^T \log \frac{q(\rvx_{t-1} | \rvx_t, \rvx_0)}{\pt(\rvx_{t-1} | \rvx_t) } - \log \pt(\rvx_0 | \rvx_1)  \right] \nn
    &= \E_{q(\rvx_{0:T})} \left[ \underbrace{\KL\left(q(\rvx_T | \rvx_0) \mid\mid p(\rvx_T)\right)}_{\Ls_T} +
    \sum_{t=2}^T \underbrace{\KL\left( q(\rvx_{t-1} | \rvx_t, \rvx_0) \mid\mid \pt(\rvx_{t-1} | \rvx_t) \right)}_{\Ls_{t-1}}
    \underbrace{- \log \pt(\rvx_0 | \rvx_1)}_{\Ls_0}
     \right]
\end{align} 

\begin{notebox}[colback=red!5]
What is the missing term to complete the bound (see VAE)?
\end{notebox}

The forward process posterior $q(\rvx_{t-1} | \rvx_t, \rvx_0)$ conditioned on $\rvx_0$ is tractable and can be compared to the learned reversed process $\pt(\rvx_{t-1} | \rvx_t)$
\begin{equation}
    q(\rvx_{t-1} | \rvx_t, \rvx_0) = \frac{q(\rvx_t | \rvx_{t-1}) q(\rvx_{t-1} | \rvx_0) }{q(\rvx_t | \rvx_0) }
\end{equation}

Pdf of exponential family distributions can be represented in a form
\begin{equation}
    p(\rvx; \veta) = h(\rvx) \exp \left( \veta^\Ts \mT(\rvx) - \mA(\veta) \right) \enspace .
\end{equation}

For multivariate Guassian distribution we get \cite{escudero_multivariate_2020}
\begin{align*}
    \dN(\rvx ; \vmu, \mSigma)
    &= (2 \pi)^{-k / 2} \det(\mSigma)^{-1 / 2} \exp \left(-\frac{1}{2}(\rvx-\vmu)^\Ts \mSigma^{-1}(\rvx - \vmu)\right) \\
    &= \exp \left(-\frac{1}{2}\rvx^\Ts \mSigma^{-1} \rvx + \rvx^\Ts \mSigma^{-1} \vmu - \frac{1}{2}\vmu^\Ts \mSigma^{-1} \vmu - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log \det(\mSigma) \right) \\
    &= \exp \left(-\frac{1}{2} \Tr(\mSigma^{-1} \rvx \rvx^\Ts ) + \Tr( \mSigma^{-1} \vmu \rvx^\Ts ) - \frac{1}{2}\vmu^\Ts \mSigma^{-1} \vmu - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log \det(\mSigma) \right) \\
    &= \exp \left(-\frac{1}{2} \vvec(\mSigma^{-1})^\Ts \vvec(\rvx \rvx^\Ts ) + (\mSigma^{-1} \vmu)^\Ts \rvx - \frac{1}{2}\vmu^\Ts \mSigma^{-1} \vmu - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log \det(\mSigma) \right) 
\end{align*}
From which we have 
\begin{align*}
    \veta &= \begin{bmatrix}
        \mSigma^{-1} \vmu \\
        -\frac{1}{2} \vvec(\mSigma^{-1})
    \end{bmatrix} \\
    T(\rvx) &= \begin{bmatrix}
        \rvx \\
        \vvec(\rvx \rvx^\Ts )
    \end{bmatrix} \\
    \mA(\veta) &= - \frac{1}{2} \left( \vmu^\Ts \mSigma^{-1} \vmu + k \log (2 \pi) + \log \det(\mSigma)\right)
\end{align*}

For the forward process we have
\begin{align*}
    q(\rvx_{t} | \rvx_{t-1}) &= \dN(\rvx_{t}; \sqrt{1-\beta_t} \rvx_{t-1}, \beta_t \mI) \\
    &= \exp \left(-\frac{1}{2\beta_t} \rvx_t^\Ts \rvx_t + \frac{\sqrt{\alpha_t}}{\beta_t} \rvx_t^\Ts \rvx_{t-1} - \frac{\alpha_t}{2\beta_t}\rvx_{t-1}^\Ts \rvx_{t-1} - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log k \beta_t \right) 
\end{align*} 

\begin{align*}
    q(\rvx_t | \rvx_0) &= \dN(\rvx_t; \sqrt{\bar{\alpha}_t} \rvx_0, (1 - \bar{\alpha}_t) \mI) \\
    &= \exp \left(-\frac{1}{2(1 - \bar{\alpha}_t)} \rvx_t^\Ts \rvx_t + \frac{\sqrt{\bar{\alpha}_t}}{(1 - \bar{\alpha}_t)} \rvx_t^\Ts \rvx_{0} - \frac{\bar{\alpha}_t}{2(1 - \bar{\alpha}_t)}\rvx_{0}^\Ts \rvx_{0} - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log k (1 - \bar{\alpha}_t) \right) 
\end{align*}

\begin{align*}
    q(\rvx_{t-1} | \rvx_0) &= \dN(\rvx_t; \sqrt{\bar{\alpha}_{t-1}} \rvx_0, (1 - \bar{\alpha}_{t-1}) \mI) \\
    &= \exp \left(-\frac{1}{2(1 - \bar{\alpha}_{t-1})} \rvx_{t-1}^\Ts \rvx_{t-1} + \frac{\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_{t-1})} \rvx_{t-1}^\Ts \rvx_{0} - \frac{\bar{\alpha}_{t-1}}{2(1 - \bar{\alpha}_{t-1})}\rvx_{0}^\Ts \rvx_{0} - \frac{k}{2} \log (2 \pi) - \frac{1}{2} \log k (1 - \bar{\alpha}_{t-1}) \right) 
\end{align*}

And hence
\begin{align*}
    q(\rvx_{t-1} | \rvx_t, \rvx_0) &= \frac{q(\rvx_t | \rvx_{t-1}) q(\rvx_{t-1} | \rvx_0) }{q(\rvx_t | \rvx_0) } \\
    &= \exp \left(
        \frac{\sqrt{\alpha_t}}{\beta_t} \rvx_t^\Ts \rvx_{t-1} - \frac{\alpha_t}{2\beta_t}\rvx_{t-1}^\Ts \rvx_{t-1}-\frac{1}{2(1 - \bar{\alpha}_{t-1})} \rvx_{t-1}^\Ts \rvx_{t-1} + \frac{\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_{t-1})} \rvx_{t-1}^\Ts \rvx_{0}
        - \mA(\veta) \right) \\
        &= \exp \left( -\frac{1}{2}\left( \frac{\alpha_t}{\beta_t} + \frac{1}{(1 - \bar{\alpha}_{t-1})} \right)\rvx_{t-1}^\Ts \rvx_{t-1}
        + \rvx_{t-1}^\Ts \left( \frac{\sqrt{\alpha_t}}{\beta_t}\rvx_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_{t-1})} \rvx_0 \right)
        - \mA(\veta) 
        \right)  \enspace .
\end{align*}
where $\mA(\veta)$ is the log-partition function (cummulant) contains all the normalizing terms not depending on $\rvx_{t-1}$.

From this we have that the covariance of the distribution $q(\rvx_{t-1} | \rvx_t, \rvx_0)$ is 
\begin{align*}
    \mSigma &= \left(\frac{\alpha_t}{\beta_t} + \frac{1}{(1 - \bar{\alpha}_{t-1})} \right)^{-1} \mI
    = \left(\frac{\beta_t + \alpha_t - \bar{\alpha}_t}{\beta_t(1 - \bar{\alpha}_{t-1})} \right)^{-1} \mI
    = \frac{\beta_t(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mI = \bar{\beta}_t \mI
    \enspace ,
\end{align*}
and the mean is
\begin{align*}
    \vmu(\rvx_t, \rvx_0) &= \left( \frac{\sqrt{\alpha_t}}{\beta_t}\rvx_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_{t-1})} \rvx_0 \right) \frac{\beta_t(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \\
    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\rvx_t
    + \frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_t)} \rvx_0
\end{align*}
So that $q(\rvx_{t-1} | \rvx_t, \rvx_0) = \dN(\rvx_{t-1}; \vmu(\rvx_t, \rvx_0), \bar{\beta}_t \mI)$.

From \eqref{eq:xt_sampling} we know that we can sample $\rvx_t$ in the forward diffusion directly from $\rvx_0$ as $\rvx_t = \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t$ with $\rvepsilon_t \sim \dN(\mathbf{0}, \mI)$. 
Hence we can also recover the $\rvx_0$ from the sample (if we know the noise) as 
$\rvx_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \rvx_t - \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t \right)$.
This also means that the posterior is
\begin{equation}\label{eq:x0_posterior}
    q(\rvx_0 | \rvx_t) = \dN(\rvx_0 ; \frac{1}{\sqrt{\bar{\alpha}_t}}\rvx_t , \frac{1-\bar{\alpha}_t}{\bar{\alpha}_t}\mI)
\end{equation}

For the mean of the distribution we thus get in terms of the known error $\rvepsilon_t$
\begin{align}\label{eq:forwardPosteriorEps}
    \vmu(\rvx_t, \rvepsilon_t) &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\rvx_t
    + \frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_t)} \rvx_0 \nn
    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\rvx_t
    + \frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{(1 - \bar{\alpha}_t)}
    \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \rvx_t - \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t  \right) \nn
    &= \frac{\alpha_t(1 - \bar{\alpha}_{t-1})+ (1 - \alpha_t)}{(1 - \bar{\alpha}_t)\sqrt{\alpha_t}}\rvx_t
    - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}}
    \rvepsilon_t \nn
    &= \frac{1}{\sqrt{\alpha_t}} \left( 
        \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \rvepsilon_t
    \right)
\end{align}
So that the posterior of the forward process can be conditionend on the known error sample $q(\rvx_{t-1} | \rvx_t, \rvepsilon_t) = \dN(\rvx_{t-1}; \vmu(\rvx_t, \rvepsilon_t), \bar{\beta}_t \mI)$.

\section{Simplifying the loss function - denoising autoencoder}

The $\Ls_T$ term in \eqref{eq:VarBoundLoss} has no learnable parameters. 
The prior $p(\rvx_T)$ is standard normal and $q(\rvx_T | \rvx_0)$ parameters depend only on the forward variance schedule through \eqref{eq:xt_sampling}.
It can just be dropped from the objective.

General the KL divergence for two multivariate Gaussians is as follows
\begin{align*}
    \KL(\dN(\vmu_q, \mSigma_q) \mid\mid \dN(\vmu_p, \mSigma_p)) = 
    \frac{1}{2} \left( \log \frac{\det(\mSigma_p)}{\det(\mSigma_q)} - k + \Tr \, (\mSigma_p^{-1} \mSigma_q) + (\vmu_q - \vmu_p)^\Ts \mSigma_p^{-1} (\vmu_q - \vmu_p) \right)
\end{align*}

For the $\Ls_{t-1}$ terms the posterior of the forward process $q(\rvx_{t-1} | \rvx_t, \rvx_0) = \dN(\rvx_{t-1}; \vmu(\rvx_t, \rvx_0), \bar{\beta}_t \mI)$ has no learnable parameters.
We fix the variance in the reverse process $\pt(\rvx_{t-1} | \rvx_t) = \dN(\rvx_{t-1}; \mt(\rvx_t, t), \st(\rvx_t, t))$ so that $\st(\rvx_t, t) = \sigma^2_t \mI$, where $\sigma^2_t$ is a known (not learnable) function of the forward variance schedule.
The minimization of the KL divergence hence simplifies to:
\begin{gather}\label{eq:Lt-1norm}
    \amint \E_{q(\rvx_0, \rvx_t)} \KL\left( q(\rvx_{t-1} | \rvx_t, \rvx_0) \mid\mid \pt(\rvx_{t-1} | \rvx_t) \right) \nn
    \text{is equivalent to} \nn
    \amint \E_{q(\rvx_0, \rvx_t)} \frac{1}{2 \sigma_t^2} \norm{\vmu(\rvx_t, \rvx_0) - \mt(\rvx_t, t)}_2^2
\end{gather}
and hence we can train $\mt$ to approximate the mean of the forward process posterior.

However, we know from \eqref{eq:forwardPosteriorEps} that mean of the forward posterior can be written with respect to $\rvx_t$ and the noise which $\rvepsilon_t$ which was used to generate if from $\rvx_0$.
\begin{equation*}
    \vmu(\rvx_t, \rvepsilon_t) = \frac{1}{\sqrt{\alpha_t}} \left( 
        \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \rvepsilon_t  \right)  
\end{equation*}
We can therefore choose the parameterization for the mean of the reverse process as
\begin{equation*}
    \mt(\rvx_t, t) = \frac{1}{\sqrt{\alpha_t}} 
    \left( \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epst(\rvx_t, t)  \right)  
\end{equation*}

In this parametrization is the reverse process
\begin{equation*}
    \pt(\rvx_{t-1} | \rvx_t) = \dN(\rvx_{t-1}; \frac{1}{\sqrt{\alpha_t}} 
    \left( \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epst(\rvx_t, t) \right), \sigma_t^2 \mI)
\end{equation*}
and we can sample $\rvx_{t-1}$ as
\begin{equation*}
    \rvx_{t-1} = \left( \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epst(\rvx_t, t) \right) + \sigma_t \rvz, \qquad \rvz \sim \dN(\mathbf{0}, \mI)
\end{equation*}

Plugging these back to the optimisation from \eqref{eq:Lt-1norm} we get
\begin{align*}
    \norm{\vmu(\rvx_t, \rvx_0) - \mt(\rvx_t, t)}_2^2 &=
    \norm{\frac{1}{\sqrt{\alpha_t}} \left( \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \rvepsilon_t  \right) 
    - \frac{1}{\sqrt{\alpha_t}} \left( \rvx_t - \frac{(1-\alpha_t)}{\sqrt{1 - \bar{\alpha}_t}} \epst(\rvx_t, t)  \right)
    }_2^2 \nn
    &= \norm{
    \frac{(1-\alpha_t)}{\sqrt{\alpha_t}\sqrt{1 - \bar{\alpha}_t}} \left( \rvepsilon_t - \epst(\rvx_t, t) \right)
    }_2^2 \nn
    &= \frac{(1-\alpha_t)^2}{\alpha_t(1 - \bar{\alpha}_t)}
    \norm{\rvepsilon_t - \epst( \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t, t) }_2^2
\end{align*}

This means that the $\Ls_{t-1}$ terms of the objective boil down to 
\begin{equation*}
    \Ls_{t-1}: \, \amint \E_{q(\rvx_0)\dN(\rvepsilon_t; \mathbf{0}, \mI)} \,
    \frac{(1-\alpha_t)^2}{2\sigma_t^2 \alpha_t(1 - \bar{\alpha}_t)} \norm{\rvepsilon_t - \epst( \underbrace{\sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t}_{\rvx_t}, t) }_2^2 \enspace ,
\end{equation*}
whereby the diffusion model is trained to predict the noise from the noised image and the corresponding timestamp.

In \cite{ho_denoising_2020} they model $\pt(\rvx_0 | \rvx_1)$ as independent discrete decoder over the image pixels. Check the paper for details.

They also found that the don't need to re-weight the loss terms so that in the end we have a simple objective
\begin{equation*}
    \Ls_{simple}: \amint \, \sum_{t=1}^T \norm{\rvepsilon_t - \epst( \underbrace{\sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t}_{\rvx_t}, t) }_2^2 \enspace ,
\end{equation*}
where the $\pt(\rvx_0 | \rvx_1)$ has been subsumed into the loss.
\begin{notebox}[colback=red!5]
    I don't quite see, how this happens but seems not very critical.
\end{notebox}

The final point here is that we can use the trained model $\epst$ to predict the original image $\widehat{\rvx}_0$ from the noised image $\rvx_t$ and the timestamp as
\begin{equation}
    \rvx_0 \approx \widehat{\rvx}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \rvx_t - \sqrt{1 - \bar{\alpha}_t} \epst (\rvx_t, t) \right) = \ut(\rvx_t, t)
\end{equation}
and we call this function $\ut(\rvx_t, t)$.

\paragraph{Some comments on this}
It is important to understand the properties of this approximator.
When $\rvx_t$ is the result of the forward process, the mean of this is
\begin{align*}
    \E(\widehat{\rvx}_0) &= \E \left[ \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t - \sqrt{1 - \bar{\alpha}_t} \epst (\sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t, t) \right) \right] \nn
    &= \rvx_0 - \frac{\sqrt{1 - \bar{\alpha}_t}}{\sqrt{\bar{\alpha}_t}} \E(\epst )
\end{align*}
Hence the bias is a function of the bias of the predictor $\epst$.

The variance 
\begin{align*}
    \Var(\widehat{\rvx}_0) &= \Var \left[ \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t - \sqrt{1 - \bar{\alpha}_t} \epst (\sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1 - \bar{\alpha}_t} \rvepsilon_t, t) \right) \right] \nn
    &= \frac{1 - \bar{\alpha}_t}{\bar{\alpha}_t} (\mI + \Var(\epst)) \enspace .
\end{align*}
It can be expected that the variance $\Var(\epst)$ is larger for bigger $t$ (further away from the original image).
The variance schedule has an effect on this through $\bar{\alpha}_t$.
\begin{notebox}[colback=red!5]
    I can't think it through but probably worse exploring a bit more.
\end{notebox}

\begin{notebox}[colback=red!5]
    When $\rvx_t$ comes from the reverse process the moments are not the same. Again, I cannot think it through now.
\end{notebox}
