\section{Measures and probability}
\begin{frame}{Measures - assigning mass to sets}

\structure{Measurable space $(S, \salg)$}
\begin{itemize}
  \item $S$ - set (e.g., $\mR^d$, discrete set, etc.)
  \item $\salg$ - $\sigma$-algebra on $S$ (collection of measurable subsets of $S$)
  \begin{itemize}
    \item closed under complements and countable unions
    \item contains $\emptyset$ and $S$
  \end{itemize}
\end{itemize}

\vskip 1em
\structure{Measure $\mu$ on $(S, \salg)$ - function $\mu: \salg \to [0, \infty]$}
\begin{itemize}
  \item $\mu(\emptyset) = 0$
  \item countable additivity: for disjoint $\{A_i: i \in I\} \subseteq \salg$, $\mu\left( \bigcup_{i \in I} A_i \right) = \sum_{i \in I} \mu(A_i)$
\end{itemize}

\vskip 1em
\structure{Examples:}
\begin{itemize}
\item counting measure: $\#(A) = $ number of elements in $A$
\item Lebesgue measure on $\mR^d$: $\lambda(A) = $ volume of $A$
\end{itemize}

\end{frame}

\note[enumerate]
{
\item $\sigma$-algebra $\salg$ is collection of measurable sets (closed under complements and countable unions)
\item For practical purposes: think of $\salg$ as "all reasonable subsets"
\item Lebesgue measure generalizes notions of length, area, volume
\item Probability measure is just a finite measure normalized to total mass 1
\item This abstraction allows us to talk about probability rigorously, but we'll soon move to more practical objects
}

\begin{frame}{Probability measure}

\structure{Probability space $(S, \salg, \mP)$}
\begin{itemize}
  \item $S$ - sample space (possible outcomes of random experiment)
  \item $\salg$ - $\sigma$-algebra on $S$ (collection of events - sets of outcomes)
  \item $\mP: \salg \to [0, 1]$ - probability measure
  \begin{itemize}
    \item \alert{normalization: $\mP(S) = 1$}
    \item countable additivity (same as before)
  \end{itemize}
\end{itemize}

\vskip 1em
\structure{Interpretation:}
\begin{itemize}
\item random experiment produces outcome $s \in S$
\item event $A \in \salg$ is set of outcomes
\item $\mP(A)$ is probability that outcome lies in $A$
\end{itemize}

\vskip 1em
\structure{Abstract formalism:} $(S, \salg, \mP)$ gives rigorous framework\\
\alert{In practice: we work with random variables and their distributions}

\end{frame}

\note[enumerate]
{
\item Probability measure is just a normalized measure - total mass equals 1
\item The abstract probability space $(S, \salg, \mP)$ is often denoted $(\Omega, \mathscr{F}, \mP)$ in literature
\item Random experiment: think coin flip, dice roll, or any random process
\item Outcome $s \in S$: specific result of the experiment (e.g., "heads", or specific image)
\item Event $A \in \salg$: set of outcomes we're interested in (e.g., "at least 2 heads in 3 flips")
\item This level of abstraction separates the experiment from what we measure
\item But as we'll see next, we usually work with random variables that map outcomes to values we care about
}

\begin{frame}{Relationship: measures and probability}

\structure{Any finite measure can be normalized:}
\begin{itemize}
\item measure space $(S, \salg, \mu)$ with $\mu(S) < \infty$
\item define $\mP(A) = \frac{\mu(A)}{\mu(S)}$ for $A \in \salg$
\item then $(S, \salg, \mP)$ is probability space
\end{itemize}

\vskip 1em
\structure{Conversely: any probability measure can be scaled}
\begin{itemize}
\item probability space $(S, \salg, \mP)$
\item for any $c > 0$, $\mu = c \cdot \mP$ is finite measure with $\mu(S) = c$
\end{itemize}

\vskip 1em
\structure{Why this matters:}
\begin{itemize}
\item energy-based models: unnormalized measures $\mu(A) = \int_A e^{-E(s)} ds$
\item normalizing gives probability: $\mP(A) = \frac{1}{Z} \int_A e^{-E(s)} ds$ where $Z = \int_S e^{-E(s)} ds$
\item change of variables preserves this relationship
\end{itemize}

\end{frame}

\note[enumerate]
{
\item Normalization is just dividing by total mass
\item This shows probability measures are special cases of finite measures
\item Connection to statistical physics and energy-based models
\item Partition function $Z$ is the normalizing constant
\item When we transform measures, we transform probabilities - this is key for flows
}