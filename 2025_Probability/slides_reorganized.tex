\section{Motivation}
\begin{frame}{Transforming distributions in generative modeling}

\structure{Core idea: transform between probability distributions}
\begin{itemize}
  \item \alert{Normalizing flows}: base distribution $\to$ data distribution (invertible)
  \item \alert{Diffusion models}: data $\to$ noise $\to$ data (forward \& reverse processes)
  \item \alert{Flow matching}: learn vector fields between distributions
\end{itemize}

\vskip 1em
\structure{Common mathematical framework:}
\begin{itemize}
  \item How does probability change under transformations?
  \item How to compute densities after transformation?
  \item What constraints do we need? (invertibility, smoothness, etc.)
\end{itemize}

\vskip 1em
\structure{Goal:} develop rigorous foundation for understanding these transformations

\end{frame}

\note[enumerate]{
\item All generative models involve transforming probability distributions
\item Different models make different trade-offs between exactness and flexibility
\item Measure theory provides the right language and tools for this
}

\section{Probability spaces}
\begin{frame}{Probability space}

\structure{$(S, \salg, \mP)$:}
\begin{itemize}
  \item measurable space $(S, \salg)$ 
  \begin{itemize}
    \item $S$ - sample space (e.g., image space, latent space)
    \item $\salg$ - $\sigma$-algebra on $S$ - collection of measurable sets (events)
  \end{itemize}
  \item probability measure $\mP$ - function $\mP: \salg \to [0, 1]$ 
  \begin{itemize}
    \item countable additivity: for countable disjoint $\{A_i: i \in I\} \subseteq \salg$, $\mP\left( \bigcup_{i \in I} A_i \right) = \sum_{i \in I} \mP(A_i)$
    \item \alert{normalization: $\mP(S) = 1$}
  \end{itemize}
\end{itemize}

\vskip 1em
\structure{Interpretation:}
\begin{itemize}
\item $\mP$ is a rigorous way to describe a probability distribution
\item Examples: $p_{\text{data}}$, $p_{\text{noise}}$, $p_{\text{model}}$
\end{itemize}
\end{frame}

\note[enumerate]
{
\item $\sigma$-algebra $\salg$ - non-empty collection of subsets closed under complement and countable unions 
\vskip -0.1em
\begin{itemize}
    \item if $A \in \salg$, then $A^c \in \salg$
    \item if $A_i \in \salg$ for $i \in I$ (countable index set), then $\bigcup_{i \in I} A_i \in \salg$
    \item in consequence $\emptyset \in \salg$ and $S \in \salg$
  \end{itemize}
\item For most practical purposes in ML, you can think of $\salg$ as "all reasonable subsets"
\item The triplet $(S, \salg, \mP)$ is a probability space
\item In generative modeling: data space, latent space, noise space are all examples of $S$
}

\begin{frame}{Positive measures - general framework}

\structure{Positive measure on $(S, \salg)$ - function $\mu: \salg \to [0, \infty]$ s.t.:}
\begin{itemize}
  \item $\mu(\emptyset) = 0$
  \item countable additivity: for countable disjoint $\{A_i: i \in I\} \subseteq \salg$, $\mu\left( \bigcup_{i \in I} A_i \right) = \sum_{i \in I} \mu(A_i) $
  \item $\Rightarrow$ measure space $(S, \salg, \mu)$
\end{itemize}

\structure{Note:} if $\mu(S) < \infty \Rightarrow (S, \salg, \mu)$ \alert{finite} measure space.

\vskip 1em
\structure{Examples of measures:}
\begin{itemize}
\item counting measure: for finite or countable $S$, $\#(A) = $ number of elements in $A \in \salg$
\item Lebesgue measure: for Euclidean $(\mR^d, \mathscr{R}^d)$, $\lambda(\mathbf{A})$ = \alert{volume} of $\mathbf{A}$
\item probability measure: finite measure $\mP$ on $(S, \salg)$ s.t. $\mP(S) = 1$
\end{itemize}

\vskip 0.5em
\structure{Key insight:} any finite measure $\mu$ with $\mu(S) < \infty$ can be normalized: $\mP(A) = \mu(A) / \mu(S)$
\end{frame}

\note[enumerate]{
\item Probability measures are special cases of positive measures
\item Lebesgue measure is the "natural" measure on Euclidean space - it measures volume
\item The normalization property connects to energy-based models and unnormalized distributions
}

\section{Random variables and distributions}
\begin{frame}{Random variables}

$(S, \salg, \mP)$ probability space, $(T, \mathscr{T})$ another measurable space

\vskip 0.5em
\structure{Random variable $X: S \to T$ - measurable function $S$ to $T$}\\
(i.e., $X^{-1}(B) \in \salg$ for all $B \in \mathscr{T}$)
\begin{itemize}
\item for outcome $s \in S$, $X$ takes value $x = X(s) \in T$
\item pre-image of $B \in \mathscr{T}$: $\{X \in B\} = X^{-1}(B) = \{s \in S: X(s) \in B\} \in \salg$
\end{itemize}

\vskip 1em
\structure{Interpretation in generative modeling:}
\begin{itemize}
\item $X$ is a \alert{transformation} between spaces
\item encoder: data space $\to$ latent space
\item decoder: latent space $\to$ data space
\item diffusion step: $x_t \to x_{t+1}$ (forward process)
\item neural network defines transformation
\end{itemize}

\end{frame}

\note[enumerate]{
\item Measurable means we can measure probabilities after transformation
\item In practice, continuous functions are measurable
\item Random variable = transformation of probability space
}

\begin{frame}{Distribution of random variable}

$(S, \salg, \mP)$ probability space, $X: S \to T$ random variable

\vskip 0.5em
\structure{Distribution of $X$:} probability measure $P_X$ on $(T, \mathscr{T})$ defined by
\[
P_X(B) = \mP(X \in B) = \mP(\{s \in S: X(s) \in B\})
\]

\vskip 0.5em
\structure{Interpretation:}
\begin{itemize}
\item $P_X$ describes where probability mass lives in $T$ after transformation
\item $P_X(B)$ = probability that $X$ takes value in set $B$
\end{itemize}

\vskip 1em
\structure{Examples in generative modeling:}
\begin{itemize}
\item latent code $z \sim p_z$ (prior distribution)
\item generated sample $x = G(z) \sim p_{\text{model}}$ (distribution of $G(z)$)
\item noisy data $x_t \sim p_t$ (distribution at diffusion time $t$)
\end{itemize}

\end{frame}

\note[enumerate]{
\item The distribution $P_X$ is completely determined by $X$ and $\mP$
\item This is the "push-forward" of $\mP$ by $X$
\item In code: we sample $s \sim \mP$, then compute $x = X(s)$, which gives samples from $P_X$
}

% \section{Push-forward measure}
% \begin{frame}{Push-forward measure}

% \structure{Setup:} $(S, \salg, \mu)$ measure space, $(T, \mathscr{T})$ measurable space, $f: S \to T$ measurable

% \vskip 0.5em
% \structure{Push-forward of $\mu$ by $f$:} measure $\nu$ on $(T, \mathscr{T})$ defined by
% \[
% \nu(B) = \mu(f^{-1}(B)), \quad B \in \mathscr{T}
% \]

% \centering
% \scalebox{0.6}{
% \begin{tikzpicture}[>=Stealth]
% % LEFT: domain S with mesh and rectangular f^{-1}(B)
% \begin{scope}[shift={(-5,0)}]
%     \node at (-1,2) {$S$};
%     % Mesh (vertical)
%     \foreach \x in {-1.5,-1,...,1}
%         \draw[gray!70] (\x,-2) -- (\x,2);
%     % Mesh (horizontal)
%     \foreach \y in {-1.5,-1,...,1.5}
%         \draw[gray!70] (-2,\y) -- (1.5,\y);
%     % Preimage region f^{-1}(B): ALIGNED WITH GRID
%     \fill[blue!25,opacity=0.5] (-1,-1) rectangle (0,1);
%     \draw[blue!60,thick] (-1,-1) rectangle (0,1);
%     \node[blue!80] at (-0.5,0) {$f^{-1}(B)$};
% \end{scope}
% % Middle arrow: f
% \draw[->,thick] (-3,0.5) -- (-1,0.5) node[midway,above] {$f: S \to T$};
% \draw[<-,thick] (-3,-0.5) -- (-1,-0.5) node[midway,above] {preimage};
% \node at (-2,-1.5) {\color{carnelian}$\mu(f^{-1}(B)) = \nu(B)$};
% % RIGHT: codomain T with warped mesh + image B
% \begin{scope}[shift={(1,0)}]
%     \node at (0,2) {$T$};
%     % Warped vertical mesh lines
%     \foreach \x in {-1.5,-1,...,1}
%     \draw[gray!70,domain=-2:2,smooth,variable=\t]
%         plot ({0.9*\x + 0.5*sin(\t r)}, {\t + 0.5*sin(\x r)});
%     % Warped horizontal mesh lines
%     \foreach \y in {-1.5,-1,...,1.5}
%     \draw[gray!70,domain=-2:1.5,smooth,variable=\t]
%         plot ({0.9*\t + 0.5*sin(\y r)}, {\y + 0.5*sin(\t r)});
%     % Image region B
%     \begin{scope}
%         \fill[blue!25,opacity=0.5]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(-1 r)}, {-1 + 0.5*sin(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0.9*0 + 0.5*sin(\y r)}, {\y + 0.5*sin(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(1 r)}, {1 + 0.5*sin(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({0.9*(-1) + 0.5*sin(\y r)}, {\y + 0.5*sin(-1 r)})
%             -- cycle;
%         \draw[blue!60,thick]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(-1 r)}, {-1 + 0.5*sin(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0.9*0 + 0.5*sin(\y r)}, {\y + 0.5*sin(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(1 r)}, {1 + 0.5*sin(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({0.9*(-1) + 0.5*sin(\y r)}, {\y + 0.5*sin(-1 r)})
%             -- cycle;
%         \node[blue!80] at (-0.4,-0.1) {$B$};
%     \end{scope}
% \end{scope}
% \end{tikzpicture}
% }

% \vskip 0.5em
% \structure{Intuition:} probability flows through transformation $f$
% \end{frame}

% \note[enumerate]{
% \item The push-forward is well-defined because $f$ is measurable
% \item Probability of landing in $B$ equals probability of starting in pre-image
% \item This is how we transform distributions in generative models
% \item Visual: regular grid in $S$ becomes warped in $T$, but probability is preserved
% }

% \begin{frame}{Composition of transformations}

% \begin{center}
% $(S, \salg, \mP)$, $(T, \mathscr{T}, P_X)$, $(U, \mathscr{U}, P_Y)$

% \vskip 0.5em
% \scalebox{0.75}{
% \begin{tikzpicture}[>=Stealth]
% % LEFT: domain S
% \begin{scope}[shift={(-5,0)}]
%     \node at (0,2) {$S$};
%     \foreach \x in {-1.5,-1,...,1}
%         \draw[gray!70] (\x,-2) -- (\x,2);
%     \foreach \y in {-1.5,-1,...,1.5}
%         \draw[gray!70] (-2,\y) -- (1.5,\y);
%     \fill[blue!25,opacity=0.5] (-1,-1) rectangle (0,1);
%     \draw[blue!60,thick] (-1,-1) rectangle (0,1);
%     \node[blue!80] at (-0.5,0) {$X^{-1}(B)$};
% \end{scope}
% % Arrow X: S -> T
% \draw[->,thick] (-3,0) -- (-1,0) node[midway,above] {$X: S \to T$} node[midway,below] {$x = X(s)$};
% % MIDDLE: T space
% \begin{scope}[shift={(1,0)}]
%     \node at (0,2) {$T$};
%     \foreach \x in {-1.5,-1,...,1}
%     \draw[gray!70,domain=-2:2,smooth,variable=\t]
%         plot ({0.9*\x + 0.5*sin(\t r)}, {\t + 0.5*sin(\x r)});
%     \foreach \y in {-1.5,-1,...,1.5}
%     \draw[gray!70,domain=-2:1.5,smooth,variable=\t]
%         plot ({0.9*\t + 0.5*sin(\y r)}, {\y + 0.5*sin(\t r)});
%     \begin{scope}
%         \fill[blue!25,opacity=0.5]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(-1 r)}, {-1 + 0.5*sin(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0.9*0 + 0.5*sin(\y r)}, {\y + 0.5*sin(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(1 r)}, {1 + 0.5*sin(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({0.9*(-1) + 0.5*sin(\y r)}, {\y + 0.5*sin(-1 r)})
%             -- cycle;
%         \draw[blue!60,thick]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(-1 r)}, {-1 + 0.5*sin(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0.9*0 + 0.5*sin(\y r)}, {\y + 0.5*sin(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({0.9*\x + 0.5*sin(1 r)}, {1 + 0.5*sin(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({0.9*(-1) + 0.5*sin(\y r)}, {\y + 0.5*sin(-1 r)})
%             -- cycle;
%         \node[blue!80] at (-0.4,-0.1) {$B$};
%     \end{scope}
% \end{scope}
% % Arrow g: T -> U
% \draw[->,thick] (2.4,0) -- (3.7,0) node[midway,above] {$g : T \to U$} node[midway,below] {$ y = g(x)$};
% % RIGHT: U space
% \begin{scope}[shift={(6,0)}]
%     \node at (-1.2,2) {$U$};
%     \foreach \x in {-1.5,-1,...,1}
%         \draw[gray!70,domain=-2:2,smooth,variable=\t]
%             plot ({\x - 0.5*sin(\t r)}, {\t + 0.5*cos(\x r)});
%     \foreach \y in {-1.5,-1,...,1.5}
%         \draw[gray!70,domain=-2:1.5,smooth,variable=\t]
%             plot ({\t - 0.5*sin(\y r)}, {\y + 0.5*cos(\t r)});
%     \begin{scope}
%         \fill[blue!25,opacity=0.5]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({\x - 0.5*sin(-1 r)}, {-1 + 0.5*cos(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0 - 0.5*sin(\y r)}, {\y + 0.5*cos(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({\x - 0.5*sin(1 r)}, {1 + 0.5*cos(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({-1 - 0.5*sin(\y r)}, {\y + 0.5*cos(-1 r)})
%             -- cycle;
%         \draw[blue!60,thick]
%             plot[domain=-1:0,smooth,variable=\x]
%                 ({\x - 0.5*sin(-1 r)}, {-1 + 0.5*cos(\x r)})
%             -- plot[domain=-1:1,smooth,variable=\y]
%                 ({0 - 0.5*sin(\y r)}, {\y + 0.5*cos(0 r)})
%             -- plot[domain=0:-1,smooth,variable=\x]
%                 ({\x - 0.5*sin(1 r)}, {1 + 0.5*cos(\x r)})
%             -- plot[domain=1:-1,smooth,variable=\y]
%                 ({-1 - 0.5*sin(\y r)}, {\y + 0.5*cos(-1 r)})
%             -- cycle;
%         \node[blue!80] at (-0.4,0.2) {$C = g(B)$};
%     \end{scope}
% \end{scope}
% \end{tikzpicture}
% }
% \end{center}

% \alert{push-forward} of $\mP$ by $X$: $P_X(B) = \mP(X \in B)$ \\
% \alert{push-forward} of $P_X$ by $g$: $P_Y(C) = P_X(g^{-1}(C))$ where $C = g(B)$

% \vskip 0.5em
% \structure{Chain rule:} $P_Y = P_{g \circ X}$ where $Y = g(X)$
% \end{frame}

% \note[enumerate]{
% \item Compositions are fundamental in deep learning - stacking layers
% \item In normalizing flows: chain multiple invertible transformations
% \item Each transformation pushes probability forward
% }

% \section{Densities and measures}
% \begin{frame}{Densities vs measures}

% \structure{Measure $\mu$:} assigns "mass" to sets, $\mu: \salg \to [0, \infty]$

% \vskip 0.5em
% \structure{Density $p$:} assigns "density" to points, $p: S \to [0, \infty)$

% \vskip 1em
% \structure{Relationship:} when density exists,
% \[
% \mu(A) = \int_A p(s) \, d\lambda(s)
% \]
% where $\lambda$ is a reference measure (e.g., Lebesgue measure)

% \vskip 1em
% \structure{Notation:} $d\mu = p \, d\lambda$ or $d\mu(s) = p(s) \, ds$

% \vskip 1em
% \structure{When does density exist?}
% \begin{itemize}
% \item $\mu$ is \alert{absolutely continuous} w.r.t. $\lambda$: $\lambda(A) = 0 \Rightarrow \mu(A) = 0$
% \item Radon-Nikodym theorem: density $p = \frac{d\mu}{d\lambda}$ exists
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item In ML we almost always work with densities, not measures directly
% \item For continuous distributions on $\mR^d$, reference measure is Lebesgue (volume)
% \item For discrete distributions, reference measure is counting measure
% \item The density is also called Radon-Nikodym derivative
% }

% \begin{frame}{What we compute in code}

% \structure{Probability measure $\mP$:} abstract object, assigns probabilities to events

% \vskip 0.5em
% \structure{Probability density $p$:} function we can evaluate and compute with

% \vskip 1em
% \structure{In practice:}
% \begin{itemize}
% \item We represent distributions via densities: $p(x)$
% \item We compute likelihoods: $p(x | \theta)$
% \item We optimize log-likelihoods: $\log p(x)$
% \item Neural networks output parameters of densities
% \end{itemize}

% \vskip 1em
% \structure{Examples:}
% \begin{itemize}
% \item Gaussian: $p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$
% \item Standard normal (base distribution): $p(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}$
% \item Model distribution: $p_\theta(x) = $ output of generative model
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item This is why we need to understand both measures and densities
% \item Measures give rigorous foundation, densities give computational tool
% \item Change of variables formula works at the density level
% }

% \section{Change of variables}
% \begin{frame}{Change of variables formula}

% \structure{Setup:} 
% \begin{itemize}
% \item $X$ has density $p_X$ on $\mR^d$ (w.r.t. Lebesgue measure)
% \item $g: \mR^d \to \mR^d$ is differentiable bijection (invertible transformation)
% \item $Y = g(X)$
% \end{itemize}

% \vskip 1em
% \structure{Change of variables formula:}
% \[
% p_Y(y) = p_X(g^{-1}(y)) \left| \det \frac{\partial g^{-1}}{\partial y}(y) \right|
% \]
% or equivalently, for $y = g(x)$:
% \[
% p_Y(g(x)) = p_X(x) \left| \det \frac{\partial g}{\partial x}(x) \right|^{-1}
% \]

% \vskip 1em
% \structure{The Jacobian matrix:} $J_g(x) = \frac{\partial g}{\partial x}(x) = \begin{bmatrix} \frac{\partial g_1}{\partial x_1} & \cdots & \frac{\partial g_1}{\partial x_d} \\ \vdots & \ddots & \vdots \\ \frac{\partial g_d}{\partial x_1} & \cdots & \frac{\partial g_d}{\partial x_d} \end{bmatrix}$

% \end{frame}

% \note[enumerate]{
% \item This is THE fundamental formula for normalizing flows
% \item The Jacobian determinant accounts for volume distortion
% \item For flows: we know $g$ (neural network), so we can compute exact likelihood
% }

% \begin{frame}{Why the Jacobian determinant?}

% \structure{Jacobian matrix $J_g(x)$:} local linear approximation of $g$ at $x$
% \[
% g(x + \delta x) \approx g(x) + J_g(x) \delta x
% \]

% \vskip 0.5em
% \structure{Determinant $\det J_g(x)$:} volume scaling factor
% \begin{itemize}
% \item small volume element $dV$ at $x$ transforms to volume $|\det J_g(x)| \, dV$ at $g(x)$
% \item if $|\det J_g| > 1$: volume expands
% \item if $|\det J_g| < 1$: volume contracts
% \end{itemize}

% \vskip 1em
% \structure{Probability conservation:} total probability must remain 1
% \[
% \int p_Y(y) \, dy = \int p_X(x) \, dx = 1
% \]
% When volume expands by factor $|\det J_g|$, density must compress by same factor!

% \vskip 0.5em
% \structure{Visual:} warped grid shows volume distortion

% \end{frame}

% \note[enumerate]{
% \item This is geometric intuition for change of variables
% \item Probability is like incompressible fluid - conserved under transformation
% \item Your warped grid diagrams perfectly illustrate this
% }

% \begin{frame}{Simple example: affine transformation}

% \structure{1D case:} $y = g(x) = ax + b$ where $a \neq 0$

% \vskip 0.5em
% \structure{Jacobian:} $\frac{dg}{dx} = a$, so $|\det J_g| = |a|$

% \vskip 0.5em
% \structure{Change of variables:}
% \[
% p_Y(y) = p_X\left(\frac{y-b}{a}\right) \frac{1}{|a|}
% \]

% \vskip 1em
% \structure{Interpretation:}
% \begin{itemize}
% \item if $|a| > 1$: stretching the space, density decreases by factor $|a|$
% \item if $|a| < 1$: compressing the space, density increases by factor $1/|a|$
% \item shift $b$ doesn't affect density, only location
% \end{itemize}

% \vskip 1em
% \structure{Example:} $X \sim \mathcal{N}(0, 1)$, $Y = 2X + 3$
% \[
% p_Y(y) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y-3)^2}{2 \cdot 4}\right) \cdot \frac{1}{2} = \frac{1}{\sqrt{2\pi \cdot 4}} \exp\left(-\frac{(y-3)^2}{8}\right)
% \]
% So $Y \sim \mathcal{N}(3, 4)$ âœ“

% \end{frame}

% \note[enumerate]{
% \item This simple example shows all the key ideas
% \item Scaling the space by $a$ scales the variance by $a^2$ but density by $1/|a|$
% \item For Gaussian: $\mathcal{N}(\mu, \sigma^2)$ has density that scales with $1/\sigma$
% }

% \begin{frame}{Multidimensional example}

% \structure{2D affine transformation:} $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ where $A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$

% \vskip 0.5em
% \structure{Jacobian:} $J_g = A$, so $|\det J_g| = |a_{11}a_{22} - a_{12}a_{21}|$

% \vskip 0.5em
% \structure{Change of variables:}
% \[
% p_{\mathbf{Y}}(\mathbf{y}) = p_{\mathbf{X}}(A^{-1}(\mathbf{y} - \mathbf{b})) \cdot \frac{1}{|\det A|}
% \]

% \vskip 1em
% \structure{Visual interpretation:}
% \begin{itemize}
% \item $A$ transforms unit square to parallelogram
% \item area of parallelogram = $|\det A|$
% \item if $|\det A| = 2$: areas double, densities halve
% \end{itemize}

% \vskip 0.5em
% \structure{Example:} $\mathbf{X} \sim \mathcal{N}(\mathbf{0}, I)$, $\mathbf{Y} = A\mathbf{X}$
% \[
% \mathbf{Y} \sim \mathcal{N}(\mathbf{0}, AA^T) \quad \text{with density} \quad p_{\mathbf{Y}}(\mathbf{y}) = \frac{1}{|\det A|} \frac{1}{2\pi} e^{-\frac{1}{2}\mathbf{y}^T(AA^T)^{-1}\mathbf{y}}
% \]

% \end{frame}

% \note[enumerate]{
% \item Affine transformations are building blocks for more complex flows
% \item Many flow architectures use affine coupling layers
% \item Determinant computation is key bottleneck - need tractable architectures
% }

% \section{Applications to generative models}
% \begin{frame}{Normalizing flows}

% \structure{Idea:} learn invertible transformation $g_\theta: \mR^d \to \mR^d$

% \vskip 0.5em
% \structure{Setup:}
% \begin{itemize}
% \item base distribution $z \sim p_Z(z)$ (e.g., standard Gaussian)
% \item transformation $x = g_\theta(z)$ (neural network)
% \item model distribution: $p_\theta(x) = p_Z(g_\theta^{-1}(x)) |\det J_{g_\theta^{-1}}(x)|$
% \end{itemize}

% \vskip 1em
% \structure{Training:} maximize log-likelihood
% \[
% \log p_\theta(x) = \log p_Z(g_\theta^{-1}(x)) + \log |\det J_{g_\theta^{-1}}(x)|
% \]

% \vskip 1em
% \structure{Requirements:}
% \begin{itemize}
% \item $g_\theta$ must be invertible (bijection)
% \item $g_\theta$ and $g_\theta^{-1}$ must be efficiently computable
% \item $\det J_{g_\theta}$ must be tractable (triangular Jacobian, etc.)
% \end{itemize}

% \vskip 0.5em
% \structure{Sampling:} $z \sim p_Z$, then $x = g_\theta(z)$ gives sample from $p_\theta$

% \end{frame}

% \note[enumerate]{
% \item Normalizing flows give exact likelihood via change of variables
% \item Different architectures make different trade-offs for tractable Jacobian
% \item Coupling layers, autoregressive flows, continuous normalizing flows, etc.
% }

% \begin{frame}{Diffusion models}

% \structure{Forward process:} sequence of transformations adding noise
% \[
% x_0 \to x_1 \to x_2 \to \cdots \to x_T
% \]
% where $x_t = \sqrt{1-\beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_t$, $\epsilon_t \sim \mathcal{N}(0, I)$

% \vskip 0.5em
% \structure{Each step:} push-forward of measure by noising transformation
% \begin{itemize}
% \item $p_t$ = distribution of $x_t$
% \item forward process has known (Gaussian) form
% \item $p_T \approx \mathcal{N}(0, I)$ for large $T$
% \end{itemize}

% \vskip 1em
% \structure{Reverse process:} learn transformations going backward
% \[
% x_T \to x_{T-1} \to \cdots \to x_1 \to x_0
% \]
% modeled by neural network $x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z$

% \vskip 0.5em
% \structure{Key difference from flows:}
% \begin{itemize}
% \item forward process not invertible (adds randomness)
% \item don't compute exact likelihood, use variational bound (ELBO)
% \item don't need tractable Jacobian
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item Diffusion uses transformations but different mathematical framework
% \item Forward process is Markov chain with known transitions
% \item Reverse process also learned as Markov chain
% \item Change of variables still relevant for understanding forward process
% }

% \begin{frame}{Flow matching}

% \structure{Idea:} learn continuous-time transformation via vector field

% \vskip 0.5em
% \structure{Setup:}
% \begin{itemize}
% \item source distribution $p_0$ (e.g., data), target distribution $p_1$ (e.g., Gaussian)
% \item vector field $v_\theta(x, t): \mR^d \times [0,1] \to \mR^d$ (neural network)
% \item flow: $\frac{d\phi_t(x)}{dt} = v_\theta(\phi_t(x), t)$ with $\phi_0(x) = x$
% \end{itemize}

% \vskip 1em
% \structure{Probability flow:} time-dependent density $p_t$ satisfies continuity equation
% \[
% \frac{\partial p_t}{\partial t} + \nabla \cdot (p_t v_\theta) = 0
% \]

% \vskip 0.5em
% \structure{Training:} match vector field to conditional flow (simulation-free)
% \[
% \mathcal{L}(\theta) = \mathbb{E}_{t, x_0 \sim p_0, x_1 \sim p_1} \| v_\theta(x_t, t) - (x_1 - x_0) \|^2
% \]

% \vskip 1em
% \structure{Connection to change of variables:}
% \begin{itemize}
% \item infinitesimal transformation: $x_{t+dt} = x_t + v_\theta(x_t, t) dt$
% \item instantaneous change of variables: $\frac{dp_t}{dt} = -p_t (\nabla \cdot v_\theta)$
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item Flow matching is continuous-time analog of discrete flows
% \item Vector field defines infinitesimal transformations
% \item Continuity equation is continuous version of change of variables
% \item Don't need invertibility or Jacobian determinant during training
% }

% \begin{frame}{Comparison of approaches}

% \begin{center}
% \small
% \begin{tabular}{lccc}
% \hline
% & \textbf{Normalizing Flows} & \textbf{Diffusion} & \textbf{Flow Matching} \\
% \hline
% Transformation & invertible $g$ & stochastic chain & vector field $v$ \\
% Invertibility & required & not required & not required \\
% Exact likelihood & yes & no (ELBO) & no \\
% Jacobian & must be tractable & not needed & not needed \\
% Training & max likelihood & ELBO & regression \\
% Sampling & $x = g(z)$ & iterative denoising & ODE integration \\
% \hline
% \end{tabular}
% \end{center}

% \vskip 1em
% \structure{Common foundation:}
% \begin{itemize}
% \item all transform between distributions
% \item push-forward measure describes how probability changes
% \item change of variables (discrete or continuous) relates densities
% \end{itemize}

% \vskip 0.5em
% \structure{Different trade-offs:}
% \begin{itemize}
% \item flows: exact likelihood but constrained architecture
% \item diffusion: flexible architecture but approximate likelihood
% \item flow matching: flexible and efficient but no likelihood
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item All three approaches share the same mathematical foundation
% \item Understanding push-forward and change of variables unifies them
% \item Choice depends on application: need likelihood? need flexibility? computational budget?
% }

% \section{Almost sure concepts}
% \begin{frame}{Null sets and almost everywhere}

% \structure{Measure space $(S, \salg, \mu)$}
% \begin{itemize}
%   \item set $A \in \salg$ with $\mu(A) = 0$ is called \alert{null set}
%   \item statement holds \alert{almost everywhere} (a.e.) if true except on null set
% \end{itemize}

% \vskip 1em
% \structure{In probability space $(S, \salg, \mP)$:}
% \begin{itemize}
% \item \alert{almost surely} (a.s.) = almost everywhere w.r.t. $\mP$
% \item null event: $\mP(A) = 0$
% \item almost sure event: $\mP(A) = 1$
% \end{itemize}

% \vskip 1em
% \structure{Why it matters in practice:}
% \begin{itemize}
% \item continuous distributions: $\mP(X = x) = 0$ for any specific $x$
% \item densities only defined almost everywhere (can change on null sets)
% \item convergence theorems: often hold almost surely
% \item numerical computation: null sets have measure zero, can ignore
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item Almost sure is the probability analog of almost everywhere
% \item Sets of measure zero don't contribute to integrals
% \item This is why we can work with densities that might be undefined at isolated points
% \item In ML: training dynamics might have measure-zero singular points, but they don't matter
% }

% \begin{frame}{Equivalence of random variables}

% \structure{Random variables $X, Y: S \to T$ are \alert{equivalent} if $\mP(X = Y) = 1$}

% \vskip 0.5em
% \structure{Notation:} $X \equiv Y$ or $X = Y$ a.s.

% \vskip 1em
% \structure{Consequences:}
% \begin{itemize}
% \item $X \equiv Y \Rightarrow P_X = P_Y$ (same distribution)
% \item $\{X \in B\}$ and $\{Y \in B\}$ differ by null set for any $B$
% \item $\mathbb{E}[f(X)] = \mathbb{E}[f(Y)]$ for any measurable $f$
% \end{itemize}

% \vskip 1em
% \structure{In generative modeling:}
% \begin{itemize}
% \item two models with same distribution are equivalent (for sampling)
% \item training objective depends only on distribution, not on specific implementation
% \item can modify model on null sets without affecting performance
% \end{itemize}

% \vskip 0.5em
% \structure{Example:} 
% \begin{itemize}
% \item density $p(x)$ and $\tilde{p}(x)$ that differ at single point $x_0$
% \item these define equivalent distributions (differ on set $\{x_0\}$ with measure 0)
% \end{itemize}

% \end{frame}

% \note[enumerate]{
% \item Equivalence is weaker than equality but strong enough for probability
% \item This is why we can have multiple valid versions of a density (differ on null sets)
% \item In practice: different implementations of same model are equivalent if they have same distribution
% }
