\section{Random variables and distributions}
\begin{frame}{Random variable - moving to value space}

\structure{Random variable $X: S \to T$}
\begin{itemize}
  \item $(S, \salg, \mP)$ - probability space (random experiment)
  \item $(T, \mathscr{T})$ - measurable space (value space, e.g., $\mR^d$)
  \item $X$ - measurable function: $X^{-1}(B) \in \salg$ for all $B \in \mathscr{T}$
\end{itemize}

\vskip 1em
\structure{Interpretation:}
\begin{itemize}
\item random experiment produces outcome $s \in S$
\item we observe value $x = X(s) \in T$ - called \alert{realization} of $X$
\item $X$ maps abstract outcomes to concrete values we care about
\end{itemize}

\vskip 1em
\structure{Notation convention:}
\begin{itemize}
\item $X$ - random variable (the function itself)
\item $x$ - realization (a specific value in $T$)
\end{itemize}

\end{frame}

\note[enumerate]
{
\item Measurability is technical requirement - ensures we can measure probabilities after transformation
\item For continuous functions between nice spaces (e.g., $\mR^d$ with Borel sets), measurability is automatic
\item The value space $T$ is what we actually care about - numbers, vectors, images, etc.
\item Abstract sample space $S$ is often just formal device
\item In deep learning: we care about generated image $x$, not the random seed that produced it
\item Capital letter = random variable (random), lowercase = realization (fixed value)
}

\begin{frame}{Example: dice sum}

\structure{Roll two dice and sum them}

\vskip 0.5em
\structure{Setup:}
\begin{itemize}
\item sample space: $S = \{(1,1), (1,2), \ldots, (6,6)\}$ (36 outcomes)
\item probability: $\mP(\{(i,j)\}) = \frac{1}{36}$ for each outcome
\item random variable: $X(s_1, s_2) = s_1 + s_2$
\item value space: $T = \{2, 3, 4, \ldots, 12\}$
\end{itemize}

\vskip 1em
\structure{Realization:}
\begin{itemize}
\item if experiment gives $(3, 5) \in S$, then $x = X(3,5) = 8$
\item $x = 8$ is a realization of random variable $X$
\end{itemize}

\vskip 1em
\structure{Key point:}
\begin{itemize}
\item we care about the sum (value in $T$), not which dice showed what (outcome in $S$)
\item this motivates working with distribution on $T$ directly
\end{itemize}

\end{frame}

\note[enumerate]
{
\item Concrete example to ground the abstraction
\item Note: multiple outcomes in $S$ can give same value in $T$ (e.g., (2,6) and (3,5) both give sum 8)
\item This is why we need the distribution $P_X$ on $T$ - it aggregates probabilities
\item Example: $P_X(\{7\}) = \frac{6}{36}$ because 6 outcomes map to sum of 7
}

\begin{frame}{Distribution of random variable}

\structure{Given: $(S, \salg, \mP)$ and random variable $X: S \to T$}

\vskip 0.5em
\structure{Distribution (law) of $X$: probability measure $P_X$ on $(T, \mathscr{T})$}
\[
P_X(B) = \mP(X \in B) = \mP(\{s \in S: X(s) \in B\}), \quad B \in \mathscr{T}
\]

\vskip 1em
\structure{Interpretation:}
\begin{itemize}
\item $P_X(B)$ = probability that $X$ takes value in set $B$
\item $P_X$ lives on value space $T$, not on sample space $S$
\item $P_X$ is the push-forward of $\mP$ by $X$
\end{itemize}

\vskip 1em
\structure{Notation:} $X \sim P_X$ means "$X$ has distribution $P_X$"

\vskip 0.5em
\structure{Key point:} \alert{$P_X$ is a probability measure on $T$ - we can work with it directly!}

\end{frame}

\note[enumerate]
{
\item The distribution $P_X$ is completely determined by $X$ and $\mP$
\item Push-forward: probability "flows" from $S$ to $T$ through $X$
\item Once we have $P_X$, we can often forget about $(S, \salg, \mP)$
\item In practice: we specify $P_X$ directly without ever mentioning $S$
\item "X ~ N(0,1)" directly specifies distribution on $\mR$, no mention of underlying experiment
}

\begin{frame}{Working directly with distributions}

\structure{Two perspectives on probability:}

\vskip 1em
\structure{Formal perspective:}
\begin{itemize}
\item start with abstract probability space $(S, \salg, \mP)$
\item define random variable $X: S \to T$
\item derive distribution $P_X$ on value space $T$
\end{itemize}

\vskip 1em
\structure{Practical perspective (what we actually do):}
\begin{itemize}
\item \alert{directly specify distribution $P_X$ on value space $T$}
\item $(S, \salg, \mP)$ is implicit, often $S = T$ and $X = $ identity
\item notation: "$X \sim \mathcal{N}(0, I)$" directly defines Gaussian on $\mR^d$
\end{itemize}

\vskip 1em
\alert{From now on: we work with distributions on value spaces $T = \mR^d$}

\end{frame}

\note[enumerate]
{
\item This is the key conceptual shift for students
\item Abstract probability space is formal machinery, but not where we actually work
\item In generative modeling: we always work directly with distributions on $\mR^d$
\item The triplet $(\Omega, \mathscr{F}, \mP)$ is rarely mentioned in ML papers
\item Examples: "$z \sim p_{\text{prior}}$", "$x \sim p_{\text{data}}$" - these directly specify distributions
\item We're working with push-forward measures, but we specify them directly
\item Next section: we'll see different ways to represent these distributions (measure, CDF, density)
}
