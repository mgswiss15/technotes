\section{Three ways to describe a distribution}
\begin{frame}{Three primary representations of a distribution}

\structure{Given random variable $X$ with values in $\mR^d$:}

\vskip 1em
\structure{1. Probability measure $P_X$} - operates on \alert{sets}
\begin{itemize}
\item $P_X(A)$ - probability that $X \in A$
\end{itemize}

\vskip 0.5em
\structure{2. Cumulative distribution function (CDF) $F_X$} - operates on \alert{points}
\begin{itemize}
\item $F_X(x) = P_X((-\infty, x])$, always exists
\end{itemize}

\vskip 0.5em
\structure{3. Probability density function (PDF) $p_X$} - operates on \alert{points}
\begin{itemize}
\item $P_X(A) = \int_A p_X(x) \, dx$ when it exists
\end{itemize}

\vskip 1em
\alert{Three views of the same distribution}\\
{\small (other representations exist: characteristic function, MGF, etc.)}

\end{frame}

\note[enumerate]
{
\item This is a key conceptual point that students often miss
\item The measure, CDF, and density (when it exists) all describe the same distribution
\item Other representations exist: characteristic function $\phi_X(t) = E[e^{itX}]$, moment generating function $M_X(t) = E[e^{tX}]$, quantile function, etc.
\item We focus on these three because: (1) measure is fundamental, (2) CDF always exists, (3) density is what we compute with in ML
\item Measure is most general, CDF always exists, density only sometimes exists
\item In ML we mostly work with densities, but need to understand when they exist
\item Component-wise for CDF in $\mR^d$: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$
}

\begin{frame}{Interlude: integration with respect to a measure}

\structure{Why we need this:} to connect measures with densities (functions)

\vskip 1em
\structure{Riemann integration (what you know):}
\[
\int_a^b f(x) \, dx = \text{area under curve}
\]

\vskip 1em
\structure{Generalization - integration w.r.t. measure $\mu$:}
\[
\int_A f \, d\mu
\]
\begin{itemize}
\item $A$ - any measurable set
\item $\mu$ - measure (weighs different parts of space)
\end{itemize}

\vskip 1em
\structure{Intuition:} weighted sum of $f$ over set $A$, weighted by measure $\mu$

\end{frame}

\note[enumerate]
{
\item Riemann integration: only over intervals, uses length/area/volume
\item Measure integration: over any measurable set, uses any measure
\item The measure $\mu$ determines how we "weigh" different parts of the space
\item This is a generalization that will let us work with probability measures
}

\begin{frame}{Interlude: Lebesgue measure and notation}

\structure{Lebesgue measure $\lambda$ on $\mR^d$}
\begin{itemize}
\item generalizes length/area/volume
\item $\lambda([a,b]) = b - a$ (length), $\lambda(A) = $ volume of $A$
\item integration: $\int_A f \, d\lambda = \int_A f(x) \, dx$ (familiar!)
\end{itemize}

\vskip 1em
\structure{Counting measure $\#$ on discrete/countable sets}
\begin{itemize}
\item $\#(A) = $ number of elements in $A$
\item integration: $\int_A f \, d\# = \sum_{x \in A} f(x)$ (discrete sum!)
\end{itemize}

\vskip 1em
\structure{Key insight:} \alert{integral notation unifies discrete and continuous}
\[
\int_A f \, d\mu \quad \begin{cases} 
= \int_A f(x) \, dx & \text{continuous (Lebesgue)} \\
= \sum_{x \in A} f(x) & \text{discrete (counting)}
\end{cases}
\]

\end{frame}

\note[enumerate]
{
\item Lebesgue measure is the "natural" measure on Euclidean space
\item For nice functions on nice sets: Riemann = Lebesgue integration
\item But Lebesgue integration works for much wider class of functions and sets
\item The notation $dx$ is shorthand for $d\lambda(x)$ (Lebesgue measure)
\item Counting measure: discrete sums become integrals!
\item Example with probability: $\int_{\mR^d} f \, dP_X = E[f(X)]$ works for both discrete and continuous X
\item This framework unifies discrete and continuous cases - no need for separate notation
}

\begin{frame}{Probability measure - the fundamental object}

\structure{Probability measure $P_X$ on $\mR^d$}
\begin{itemize}
\item assigns probability to measurable sets: $P_X(A) = \mP(X \in A)$
\item as integration: $P_X(A) = \int_A \, dP_X$
\end{itemize}

\vskip 1em
\structure{Key properties:}
\begin{itemize}
\item normalization: $P_X(\mR^d) = \int_{\mR^d} \, dP_X = 1$
\item for continuous distributions: $P_X(\{x_0\}) = \int_{\{x_0\}} \, dP_X = 0$ (single points have zero probability)
\end{itemize}

\vskip 1em
\structure{Properties:}
\begin{itemize}
\item most fundamental representation
\item works for discrete, continuous, and mixed distributions
\end{itemize}

\end{frame}

\note[enumerate]
{
\item We work with measurable sets - for practical purposes, "all reasonable subsets"
\item Integration w.r.t. probability measure uses the framework from the interlude
\item The normalization property connects to what we saw in Section 1: any finite measure can be normalized
\item The notation $\int_A \, dP_X$ means $\int_A 1 \, dP_X$ - we're integrating the constant function 1 over set $A$, which gives the total probability mass in $A$
\item Measure is the fundamental object, but we often use CDF or density for computation
\item for continuous distributions: $P_X(\{x_0\}) = 0$ (defining property: no point masses)
\item Intuition: $P_X(\{x_0\}) = \int_{\{x_0\}} \, dP_X = 0$ because the set has zero measure
\item This is why we can't talk about "probability at a point" for continuous distributions - only density
}

\begin{frame}{Cumulative distribution function (CDF)}

\structure{CDF $F_X: \mR \to [0, 1]$}

\vskip 0.5em
\structure{Definition:}
\[
F_X(x) = P_X((-\infty, x]) = \int_{(-\infty, x]} \, dP_X
\]

\vskip 1em
\structure{Key properties:}
\begin{itemize}
\item always exists (for any distribution)
\item $F_X(-\infty) = 0$, $F_X(\infty) = 1$
\item non-decreasing, right-continuous
\item recovers probabilities: $P_X((a, b]) = F_X(b) - F_X(a)$
\end{itemize}

\vskip 1em
\structure{Note:} for $\mR^d$: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$

\end{frame}

\note[enumerate]
{
\item CDF is unique representation - different distributions have different CDFs
\item Definition uses measure integration: integral of constant function 1 over the interval
\item For multidimensional case, CDF is defined component-wise
\item CDF can have jumps (discrete distributions) or be smooth (continuous distributions)
\item Derivative of CDF gives density (when density exists): $p_X(x) = \frac{dF_X}{dx}$
\item In practice: we rarely work with CDF directly in ML, but it's theoretically important
}

\begin{frame}{Probability density function (PDF)}

\structure{PDF $p_X: \mR^d \to [0, \infty)$}

\vskip 0.5em
\structure{Definition via Radon-Nikodym derivative:}
\[
p_X = \frac{dP_X}{d\lambda}
\]
where $\lambda$ is Lebesgue measure (volume)

\vskip 0.5em
\alert{Note:} density does not always exist (requires absolute continuity)

\vskip 1em
\structure{Defining property:} for any set $A$,
\[
P_X(A) = \int_A p_X \, d\lambda = \int_A p_X(x) \, dx
\]

\vskip 1em
\structure{Notation:} $dP_X = p_X \, d\lambda$ or $dP_X(x) = p_X(x) \, dx$

\vskip 0.5em
Lebesgue measure is "weighted" by $p_X$ to give $P_X$

\end{frame}

\note[enumerate]
{
\item Radon-Nikodym derivative: formal way to define density as derivative of one measure w.r.t. another
\item The defining property says: if we integrate the density, we get the probability measure
\item This is analogous to: $df = f'(x) dx$ in calculus
\item Next slide: how to understand $p_X(x)$ at a point
}
\begin{frame}{Density at a point - the key subtlety}

\structure{Question:} We said $P_X(\{x\}) = 0$ for single points. So what is $p_X(x)$?

\vskip 1em
\structure{Answer:} $p_X(x)$ is defined as a limit:
\[
p_X(x) = \lim_{\epsilon \to 0} \frac{P_X(B_\epsilon(x))}{\lambda(B_\epsilon(x))}
\]
where $B_\epsilon(x)$ is a small ball of radius $\epsilon$ around $x$

\vskip 1em
\structure{Connection to Radon-Nikodym derivative:}
\[
p_X(x) = \frac{dP_X}{d\lambda}(x) = \lim_{\epsilon \to 0} \frac{P_X(B_\epsilon(x))}{\lambda(B_\epsilon(x))}
\]
This is literally the derivative of measure $P_X$ w.r.t. measure $\lambda$ at point $x$

\vskip 0.5em
\alert{Density exists at a point, but probability of a point is zero!}

\end{frame}

\note[enumerate]
{
\item This limit IS the Radon-Nikodym derivative - now concrete!
\item The notation $\frac{dP_X}{d\lambda}$ means exactly this: derivative of one measure w.r.t. another
\item In 1D: $B_\epsilon(x) = [x-\epsilon, x+\epsilon]$, $\lambda(B_\epsilon(x)) = 2\epsilon$
\item So $p_X(x) = \lim_{\epsilon \to 0} \frac{P_X([x-\epsilon, x+\epsilon])}{2\epsilon}$ - this looks like the derivative from calculus!
\item In higher dimensions: ball has volume proportional to $\epsilon^d$
\item This is why density can be $> 1$: it's a rate (derivative), not a probability
\item Common mistake: confusing $p_X(x)$ (density, can be $> 1$) with $P_X(\{x\})$ (probability, equals 0)
\item Interpretation: $p_X(x)$ measures local concentration of probability
}

\begin{frame}{Relationship between the three}

\structure{Comparison of the three representations:}

\vskip 0.5em
\begin{center}
\small
\begin{tabular}{lccc}
\hline
& \textbf{Measure $P_X$} & \textbf{CDF $F_X$} & \textbf{PDF $p_X$} \\
\hline
Always exists? & yes & yes & no \\
Domain & sets & points & points \\
Range & $[0,1]$ & $[0,1]$ & $[0,\infty)$ \\
\hline
\end{tabular}
\end{center}

\vskip 1em
\structure{How to convert between them:}

\vskip 0.5em
\begin{center}
\small
\begin{tabular}{lll}
\hline
\textbf{From} & \textbf{To} & \textbf{Formula} \\
\hline
$P_X$ & $F_X$ & $F_X(x) = P_X((-\infty, x])$ \\
$P_X$ & $p_X$ & $p_X = \frac{dP_X}{d\lambda}$ (if abs. cont.) \\
$p_X$ & $P_X$ & $P_X(A) = \int_A p_X \, d\lambda$ \\
$p_X$ & $F_X$ & $F_X(x) = \int_{-\infty}^x p_X(t) \, dt$ \\
$F_X$ & $p_X$ & $p_X(x) = \frac{dF_X}{dx}$ (if smooth) \\
\hline
\end{tabular}
\end{center}

\end{frame}

\note[enumerate]
{
\item Measure is most fundamental but assigns probabilities to sets, not points
\item CDF always exists and can be evaluated at points
\item PDF only exists for absolutely continuous distributions
\item In ML: we work with PDFs (assume absolute continuity)
\item PDF is most convenient: can evaluate at points and optimize
}
