\section{Three ways to describe a distribution}
\begin{frame}{Three primary representations of a distribution}

\structure{Given random variable $X$ with values in $\mR^d$, we focus on three primary ways to describe its distribution:}

\vskip 1em
\structure{1. Probability measure $P_X$}
\begin{itemize}
\item assigns probabilities to sets: $P_X(A)$ for $A \subseteq \mR^d$
\item most fundamental representation
\end{itemize}

\vskip 0.5em
\structure{2. Cumulative distribution function (CDF) $F_X$}
\begin{itemize}
\item $F_X(x) = P_X((-\infty, x])$ (for $x \in \mR^d$, component-wise)
\item always exists
\end{itemize}

\vskip 0.5em
\structure{3. Probability density function (PDF) $p_X$}
\begin{itemize}
\item when it exists: $P_X(A) = \int_A p_X(x) \, dx$
\item does not always exist!
\end{itemize}

\vskip 1em
\alert{These are three different ways to represent the same probabilistic object}

\end{frame}

\note[enumerate]
{
\item This is a key conceptual point that students often miss
\item The measure, CDF, and density (when it exists) all describe the same distribution
\item Other representations exist: characteristic function $\phi_X(t) = E[e^{itX}]$, moment generating function $M_X(t) = E[e^{tX}]$, quantile function, etc.
\item We focus on these three because: (1) measure is fundamental, (2) CDF always exists, (3) density is what we compute with in ML
\item Measure is most general, CDF always exists, density only sometimes exists
\item In ML we mostly work with densities, but need to understand when they exist
\item Component-wise for CDF in $\mR^d$: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$
}

\begin{frame}{Probability measure - the fundamental object}

\structure{Probability measure $P_X: \mathscr{B}(\mR^d) \to [0,1]$}

\vskip 0.5em
\structure{Definition:}
\begin{itemize}
\item assigns probability to measurable sets $A \in \mathscr{B}(\mR^d)$ (Borel sets)
\item $P_X(A) = \mP(X \in A) = $ probability that $X$ takes value in $A$
\end{itemize}

\vskip 1em
\structure{Example: Gaussian $X \sim \mathcal{N}(0, 1)$ on $\mR$}
\begin{itemize}
\item $P_X([0, 1]) = \int_0^1 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \, dx \approx 0.34$
\item $P_X(\mR) = 1$
\item $P_X(\{0\}) = 0$ (single point has zero probability)
\end{itemize}

\vskip 1em
\structure{Properties:}
\begin{itemize}
\item most general representation
\item works for discrete, continuous, and mixed distributions
\item but often hard to work with directly
\end{itemize}

\end{frame}

\note[enumerate]
{
\item Borel sets are generated by open sets - think of them as "all reasonable subsets"
\item For continuous distributions, individual points have zero measure
\item Measure is the fundamental object, but we often use CDF or density for computation
\item Example calculation shows that we integrate the density to get the measure
}

\begin{frame}{Cumulative distribution function (CDF)}

\structure{CDF $F_X: \mR^d \to [0, 1]$}

\vskip 0.5em
\structure{Definition (for $\mR$):}
\[
F_X(x) = P_X((-\infty, x]) = \mP(X \leq x)
\]

\vskip 1em
\structure{Example: Gaussian $X \sim \mathcal{N}(0, 1)$}
\[
F_X(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, dt = \Phi(x)
\]
\begin{itemize}
\item $F_X(0) = 0.5$ (half the probability below 0)
\item $F_X(-\infty) = 0$, $F_X(\infty) = 1$
\end{itemize}

\vskip 1em
\structure{Properties:}
\begin{itemize}
\item always exists (for any distribution)
\item non-decreasing, right-continuous
\item $P_X((a, b]) = F_X(b) - F_X(a)$
\end{itemize}

\end{frame}

\note[enumerate]
{
\item CDF is unique representation - different distributions have different CDFs
\item For multidimensional case: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$
\item CDF can have jumps (discrete distributions) or be continuous
\item Derivative of CDF gives density (when density exists)
\item In practice: we rarely work with CDF directly in ML, but it's theoretically important
}

\begin{frame}{Integration with respect to a measure}

\structure{From Riemann to measure integration}

\vskip 0.5em
\structure{Riemann integration (what you know):}
\[
\int_a^b f(x) \, dx = \text{area under curve from } a \text{ to } b
\]

\vskip 1em
\structure{Integration w.r.t. measure $\mu$ (generalization):}
\[
\int_A f \, d\mu = \int_A f(x) \, \mu(dx) = \text{weighted sum/integral over set } A
\]
\begin{itemize}
\item $A$ - any measurable set (not just intervals)
\item $\mu$ - measure that "weighs" different parts of space
\item generalizes Riemann integration
\end{itemize}

\vskip 1em
\structure{Special case - Lebesgue measure $\lambda$ on $\mR^d$:}
\[
\int_A f \, d\lambda = \int_A f(x) \, dx \quad \text{(familiar notation!)}
\]
where $d\lambda = dx$ is the "volume element"

\end{frame}

\note[enumerate]
{
\item Riemann: integrate over intervals, $dx$ is infinitesimal length
\item Lebesgue: integrate over any measurable set, still use $dx$ notation
\item General measure: $d\mu$ replaces $dx$, measure weighs space differently
\item Example: counting measure $\#$: $\int_A f \, d\# = \sum_{x \in A} f(x)$ (discrete sum)
\item Example: probability measure $\mP$: $\int_A f \, d\mP = E[f(X) \mathbf{1}_{X \in A}]$ (expectation)
\item Notation variants: $\int f d\mu = \int f(x) \mu(dx) = \int f(x) d\mu(x)$ all mean the same
\item This generalizes integration to work with any measure, not just "area" or "volume"
}

\begin{frame}{Probability density function (PDF)}

\structure{PDF $p_X: \mR^d \to [0, \infty)$}

\vskip 0.5em
\structure{Definition:} when it exists,
\[
P_X(A) = \int_A p_X \, d\lambda = \int_A p_X(x) \, dx
\]
where $\lambda$ is Lebesgue measure (volume)

\vskip 0.5em
\structure{Interpretation:}
\begin{itemize}
\item $p_X$ is \alert{density} - probability per unit volume
\item $P_X$ is "weighted" by density: $dP_X = p_X \, d\lambda$ or $dP_X(x) = p_X(x) \, dx$
\item $p_X = \frac{dP_X}{d\lambda}$ - Radon-Nikodym derivative
\end{itemize}

\vskip 1em
\structure{Example: Gaussian $X \sim \mathcal{N}(0, 1)$}
\[
p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}, \quad P_X([a,b]) = \int_a^b p_X(x) \, dx
\]
note: $p_X(0) \approx 0.40$ can be $> 1$ (it's density, not probability!)

\vskip 1em
\structure{When does density exist?} $P_X$ absolutely continuous w.r.t. $\lambda$

\end{frame}

\note[enumerate]
{
\item Density is what we actually compute and optimize in ML
\item Density is NOT probability - it's probability per unit volume
\item Common mistake: thinking $p(x)$ must be $\leq 1$. Not true!
\item Radon-Nikodym theorem guarantees density exists when absolutely continuous
\item Discrete distributions (point masses) do not have densities
\item Mixed distributions (part discrete, part continuous) do not have densities
}

\begin{frame}{Relationship between the three}

\structure{How they relate:}

\vskip 0.5em
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node (measure) {Probability measure $P_X$};
\node (cdf) [below left of=measure, node distance=3cm] {CDF $F_X$};
\node (pdf) [below right of=measure, node distance=3cm] {PDF $p_X$};

\draw[->] (measure) -- (cdf) node[midway,above left] {always};
\draw[->] (measure) -- (pdf) node[midway,above right] {if abs. continuous};
\draw[->] (pdf) -- (cdf) node[midway,below] {$F_X(x) = \int_{-\infty}^x p_X(t) dt$};
\draw[->] (cdf) -- (pdf) node[midway,above] {$p_X(x) = \frac{dF_X}{dx}$ (if exists)};
\draw[->] (pdf) -- (measure) node[midway,below right] {$P_X(A) = \int_A p_X(x) dx$};
\draw[->] (cdf) -- (measure) node[midway,below left] {$P_X((a,b]) = F_X(b) - F_X(a)$};
\end{tikzpicture}
\end{center}

\vskip 1em
\structure{Key points:}
\begin{itemize}
\item measure $\to$ CDF: always possible
\item measure $\to$ PDF: only when absolutely continuous
\item in ML: we almost always work with PDFs (assume absolute continuity)
\end{itemize}

\end{frame}

\note[enumerate]
{
\item This diagram shows all the relationships
\item Most fundamental: measure
\item Most general representation that always exists: CDF
\item Most convenient for computation: PDF (when it exists)
\item In deep learning: we assume our distributions have densities
\item This is reasonable for continuous distributions on $\mR^d$
}

\begin{frame}{Examples: discrete, continuous, mixed}

\structure{Discrete: $X \in \{0, 1, 2\}$ with $P_X(\{k\}) = 1/3$}
\begin{itemize}
\item measure: $P_X(\{0\}) = P_X(\{1\}) = P_X(\{2\}) = 1/3$
\item CDF: $F_X(x) = \begin{cases} 0 & x < 0 \\ 1/3 & 0 \leq x < 1 \\ 2/3 & 1 \leq x < 2 \\ 1 & x \geq 2 \end{cases}$ (step function)
\item PDF: \alert{does not exist} (point masses)
\end{itemize}

\vskip 1em
\structure{Continuous: $X \sim \mathcal{N}(0, 1)$}
\begin{itemize}
\item measure: $P_X(A) = \int_A \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx$
\item CDF: $F_X(x) = \Phi(x)$ (smooth, no jumps)
\item PDF: $p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$ \alert{exists}
\end{itemize}

\vskip 0.5em
\structure{Mixed: half point mass at 0, half uniform on $[0,1]$}
\begin{itemize}
\item PDF: \alert{does not exist} (has point mass at 0)
\end{itemize}

\end{frame}

\note[enumerate]
{
\item These examples show when density exists and when it doesn't
\item Discrete: CDF has jumps, no density
\item Continuous: CDF is smooth, density exists
\item Mixed: combination, no density
\item In generative modeling: we work with continuous distributions, so densities exist
\item Point masses and discrete distributions don't have densities w.r.t. Lebesgue measure
}
