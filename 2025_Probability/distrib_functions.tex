\section{Three ways to describe a distribution}
\begin{frame}{Three primary representations of a distribution}

\structure{Given random variable $X$ with values in $\mR^d$:}

\vskip 1em
\structure{1. Probability measure $P_X$}
\begin{itemize}
\item $P_X(A)$ - probability that $X \in A$
\item most fundamental
\end{itemize}

\vskip 0.5em
\structure{2. Cumulative distribution function (CDF) $F_X$}
\begin{itemize}
\item $F_X(x) = P_X((-\infty, x])$
\item always exists
\end{itemize}

\vskip 0.5em
\structure{3. Probability density function (PDF) $p_X$}
\begin{itemize}
\item $P_X(A) = \int_A p_X(x) \, dx$ when it exists
\item does not always exist
\end{itemize}

\vskip 1em
\alert{Three views of the same distribution}\\
{\small (other representations exist: characteristic function, MGF, etc.)}

\end{frame}

\note[enumerate]
{
\item This is a key conceptual point that students often miss
\item The measure, CDF, and density (when it exists) all describe the same distribution
\item Other representations exist: characteristic function $\phi_X(t) = E[e^{itX}]$, moment generating function $M_X(t) = E[e^{tX}]$, quantile function, etc.
\item We focus on these three because: (1) measure is fundamental, (2) CDF always exists, (3) density is what we compute with in ML
\item Measure is most general, CDF always exists, density only sometimes exists
\item In ML we mostly work with densities, but need to understand when they exist
\item Component-wise for CDF in $\mR^d$: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$
}

\begin{frame}{Interlude: integration with respect to a measure}

\structure{Why we need this:} to connect measures with densities (functions)

\vskip 1em
\structure{Riemann integration (what you know):}
\[
\int_a^b f(x) \, dx = \text{area under curve}
\]

\vskip 1em
\structure{Generalization - integration w.r.t. measure $\mu$:}
\[
\int_A f \, d\mu
\]
\begin{itemize}
\item $A$ - any measurable set
\item $\mu$ - measure (weighs different parts of space)
\end{itemize}

\vskip 1em
\structure{Intuition:} weighted sum of $f$ over set $A$, weighted by measure $\mu$

\end{frame}

\note[enumerate]
{
\item Riemann integration: only over intervals, uses length/area/volume
\item Measure integration: over any measurable set, uses any measure
\item The measure $\mu$ determines how we "weigh" different parts of the space
\item This is a generalization that will let us work with probability measures
}

\begin{frame}{Interlude: Lebesgue measure and notation}

\structure{Lebesgue measure $\lambda$ on $\mR^d$}
\begin{itemize}
\item generalizes length/area/volume
\item $\lambda([a,b]) = b - a$ (length), $\lambda(A) = $ volume of $A$
\item integration: $\int_A f \, d\lambda = \int_A f(x) \, dx$ (familiar!)
\end{itemize}

\vskip 1em
\structure{Counting measure $\#$ on discrete/countable sets}
\begin{itemize}
\item $\#(A) = $ number of elements in $A$
\item integration: $\int_A f \, d\# = \sum_{x \in A} f(x)$ (discrete sum!)
\end{itemize}

\vskip 1em
\structure{Key insight:} \alert{integral notation unifies discrete and continuous}
\[
\int_A f \, d\mu \quad \begin{cases} 
= \int_A f(x) \, dx & \text{continuous (Lebesgue)} \\
= \sum_{x \in A} f(x) & \text{discrete (counting)}
\end{cases}
\]

\end{frame}

\note[enumerate]
{
\item Lebesgue measure is the "natural" measure on Euclidean space
\item For nice functions on nice sets: Riemann = Lebesgue integration
\item But Lebesgue integration works for much wider class of functions and sets
\item The notation $dx$ is shorthand for $d\lambda(x)$ (Lebesgue measure)
\item Counting measure: discrete sums become integrals!
\item Example with probability: $\int_{\mR^d} f \, dP_X = E[f(X)]$ works for both discrete and continuous X
\item This framework unifies discrete and continuous cases - no need for separate notation
}

\begin{frame}{Probability measure - the fundamental object}

\structure{Probability measure $P_X$ on $\mR^d$}
\begin{itemize}
\item assigns probability to measurable sets: $P_X(A) = \mP(X \in A)$
\item as integration: $P_X(A) = \int_A \, dP_X$
\end{itemize}

\vskip 1em
\structure{Key properties:}
\begin{itemize}
\item normalization: $P_X(\mR^d) = \int_{\mR^d} \, dP_X = 1$
\item for continuous distributions: $P_X(\{x_0\}) = \int_{\{x_0\}} \, dP_X = 0$ (single points have zero probability)
\end{itemize}

\vskip 1em
\structure{Properties:}
\begin{itemize}
\item most fundamental representation
\item works for discrete, continuous, and mixed distributions
\end{itemize}

\end{frame}

\note[enumerate]
{
\item We work with measurable sets - for practical purposes, "all reasonable subsets"
\item Single points have zero probability for continuous distributions because they have zero volume (Lebesgue measure)
\item Intuition: $P_X(\{x_0\}) = \int_{\{x_0\}} \, dP_X = 0$ because the set has zero measure
\item This is why we can't talk about "probability at a point" for continuous distributions - only density
\item Integration w.r.t. probability measure uses the framework from the interlude
\item The normalization property connects to what we saw in Section 1: any finite measure can be normalized
\item Measure is the fundamental object, but we often use CDF or density for computation
}

\begin{frame}{Cumulative distribution function (CDF)}

\structure{CDF $F_X: \mR^d \to [0, 1]$}

\vskip 0.5em
\structure{Definition (for $\mR$):}
\[
F_X(x) = P_X((-\infty, x]) = \mP(X \leq x)
\]

\vskip 1em
\structure{Example: Gaussian $X \sim \mathcal{N}(0, 1)$}
\[
F_X(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, dt = \Phi(x)
\]
\begin{itemize}
\item $F_X(0) = 0.5$ (half the probability below 0)
\item $F_X(-\infty) = 0$, $F_X(\infty) = 1$
\end{itemize}

\vskip 1em
\structure{Properties:}
\begin{itemize}
\item always exists (for any distribution)
\item non-decreasing, right-continuous
\item $P_X((a, b]) = F_X(b) - F_X(a)$
\end{itemize}

\end{frame}

\note[enumerate]
{
\item CDF is unique representation - different distributions have different CDFs
\item For multidimensional case: $F_X(x_1, \ldots, x_d) = P_X((-\infty, x_1] \times \cdots \times (-\infty, x_d])$
\item CDF can have jumps (discrete distributions) or be continuous
\item Derivative of CDF gives density (when density exists)
\item In practice: we rarely work with CDF directly in ML, but it's theoretically important
}

\begin{frame}{Probability density function (PDF)}

\structure{PDF $p_X: \mR^d \to [0, \infty)$}

\vskip 0.5em
\structure{Definition:} when it exists,
\[
P_X(A) = \int_A p_X \, d\lambda = \int_A p_X(x) \, dx
\]
where $\lambda$ is Lebesgue measure (volume)

\vskip 0.5em
\structure{Interpretation:}
\begin{itemize}
\item $p_X$ is \alert{density} - probability per unit volume
\item $P_X$ is "weighted" by density: $dP_X = p_X \, d\lambda$ or $dP_X(x) = p_X(x) \, dx$
\item $p_X = \frac{dP_X}{d\lambda}$ - Radon-Nikodym derivative
\end{itemize}

\vskip 1em
\structure{Example: Gaussian $X \sim \mathcal{N}(0, 1)$}
\[
p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}, \quad P_X([a,b]) = \int_a^b p_X(x) \, dx
\]
note: $p_X(0) \approx 0.40$ can be $> 1$ (it's density, not probability!)

\vskip 1em
\structure{When does density exist?} $P_X$ absolutely continuous w.r.t. $\lambda$

\end{frame}

\note[enumerate]
{
\item Density is what we actually compute and optimize in ML
\item Density is NOT probability - it's probability per unit volume
\item Common mistake: thinking $p(x)$ must be $\leq 1$. Not true!
\item Radon-Nikodym theorem guarantees density exists when absolutely continuous
\item Discrete distributions (point masses) do not have densities
\item Mixed distributions (part discrete, part continuous) do not have densities
}

\begin{frame}{Relationship between the three}

\structure{How they relate:}

\vskip 0.5em
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node (measure) {Probability measure $P_X$};
\node (cdf) [below left of=measure, node distance=3cm] {CDF $F_X$};
\node (pdf) [below right of=measure, node distance=3cm] {PDF $p_X$};

\draw[->] (measure) -- (cdf) node[midway,above left] {always};
\draw[->] (measure) -- (pdf) node[midway,above right] {if abs. continuous};
\draw[->] (pdf) -- (cdf) node[midway,below] {$F_X(x) = \int_{-\infty}^x p_X(t) dt$};
\draw[->] (cdf) -- (pdf) node[midway,above] {$p_X(x) = \frac{dF_X}{dx}$ (if exists)};
\draw[->] (pdf) -- (measure) node[midway,below right] {$P_X(A) = \int_A p_X(x) dx$};
\draw[->] (cdf) -- (measure) node[midway,below left] {$P_X((a,b]) = F_X(b) - F_X(a)$};
\end{tikzpicture}
\end{center}

\vskip 1em
\structure{Key points:}
\begin{itemize}
\item measure $\to$ CDF: always possible
\item measure $\to$ PDF: only when absolutely continuous
\item in ML: we almost always work with PDFs (assume absolute continuity)
\end{itemize}

\end{frame}

\note[enumerate]
{
\item This diagram shows all the relationships
\item Most fundamental: measure
\item Most general representation that always exists: CDF
\item Most convenient for computation: PDF (when it exists)
\item In deep learning: we assume our distributions have densities
\item This is reasonable for continuous distributions on $\mR^d$
}

\begin{frame}{Examples: discrete, continuous, mixed}

\structure{Discrete: $X \in \{0, 1, 2\}$ with $P_X(\{k\}) = 1/3$}
\begin{itemize}
\item measure: $P_X(\{0\}) = P_X(\{1\}) = P_X(\{2\}) = 1/3$
\item CDF: $F_X(x) = \begin{cases} 0 & x < 0 \\ 1/3 & 0 \leq x < 1 \\ 2/3 & 1 \leq x < 2 \\ 1 & x \geq 2 \end{cases}$ (step function)
\item PDF: \alert{does not exist} (point masses)
\end{itemize}

\vskip 1em
\structure{Continuous: $X \sim \mathcal{N}(0, 1)$}
\begin{itemize}
\item measure: $P_X(A) = \int_A \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx$
\item CDF: $F_X(x) = \Phi(x)$ (smooth, no jumps)
\item PDF: $p_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$ \alert{exists}
\end{itemize}

\vskip 0.5em
\structure{Mixed: half point mass at 0, half uniform on $[0,1]$}
\begin{itemize}
\item PDF: \alert{does not exist} (has point mass at 0)
\end{itemize}

\end{frame}

\note[enumerate]
{
\item These examples show when density exists and when it doesn't
\item Discrete: CDF has jumps, no density
\item Continuous: CDF is smooth, density exists
\item Mixed: combination, no density
\item In generative modeling: we work with continuous distributions, so densities exist
\item Point masses and discrete distributions don't have densities w.r.t. Lebesgue measure
}
