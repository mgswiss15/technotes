@inproceedings{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
	pages = {6840--6851},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	date = {2020},
	keywords = {github, {inProgress}},
	file = {arXiv.org Snapshot:/home/magda/Zotero/storage/S5RD4ZI2/2006.html:text/html;Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf:/home/magda/Zotero/storage/2JZF4Q46/Ho et al_2020_Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion Models Beat {GANs} on Image Synthesis},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	pages = {8780--8794},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	urldate = {2023-05-13},
	date = {2021},
	keywords = {{notesPending}},
	file = {arXiv.org Snapshot:/home/magda/Zotero/storage/TLY8HBTD/2105.html:text/html;Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:/home/magda/Zotero/storage/5Y566YEE/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;Dhariwal and Nichol - 2021 - Supplement.pdf:/home/magda/Zotero/storage/FZQHSIDQ/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf},
}

@inproceedings{song_denoising_2021,
	title = {Denoising Diffusion Implicit Models},
	url = {https://openreview.net/forum?id=St1giarCHLP},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models ({DDIMs}), a more efficient class of iterative implicit probabilistic models with the same training procedure as {DDPMs}. In {DDPMs}, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize {DDPMs} via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that {DDIMs} can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to {DDPMs}, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.},
	eventtitle = {International Conference on Learning Representations},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	urldate = {2023-05-18},
	date = {2021-01-12},
	langid = {english},
	file = {Song et al_2021_Denoising Diffusion Implicit Models.pdf:/home/magda/Zotero/storage/U8DWRLL5/Song et al_2021_Denoising Diffusion Implicit Models.pdf:application/pdf},
}

@inproceedings{nichol_improved_2021,
	title = {Improved Denoising Diffusion Probabilistic Models},
	url = {https://proceedings.mlr.press/v139/nichol21a.html},
	abstract = {Denoising diffusion probabilistic models ({DDPM}) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, {DDPMs} can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well {DDPMs} and {GANs} cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8162--8171},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
	urldate = {2023-05-18},
	date = {2021-07-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Nichol_Dhariwal_2021_Improved Denoising Diffusion Probabilistic Models.pdf:/home/magda/Zotero/storage/HFZDWNTY/Nichol_Dhariwal_2021_Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf;Supplementary PDF:/home/magda/Zotero/storage/5PYNUC7B/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2256--2265},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	urldate = {2022-01-26},
	date = {2015-06-01},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	keywords = {skimmed},
	file = {Sohl-Dickstein et al_2015_Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf:/home/magda/Zotero/storage/H3R6MGSC/Sohl-Dickstein et al_2015_Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf:application/pdf;sohl-dickstein15-supp.pdf:/home/magda/Zotero/storage/C8NAJIR8/sohl-dickstein15-supp.pdf:application/pdf},
}

@misc{song_score-based_2021,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	number = {{arXiv}:2011.13456},
	publisher = {{arXiv}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2022-11-21},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2011.13456 [cs, stat]},
	file = {arXiv.org Snapshot:/home/magda/Zotero/storage/NJ3UR527/2011.html:text/html;Song et al_2021_Score-Based Generative Modeling through Stochastic Differential Equations.pdf:/home/magda/Zotero/storage/558IYVHH/Song et al_2021_Score-Based Generative Modeling through Stochastic Differential Equations.pdf:application/pdf},
}

@inproceedings{song_improved_2020,
	title = {Improved Techniques for Training Score-Based Generative Models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/92c3b916311a5517d9290576e3ea37ad-Abstract.html},
	abstract = {Score-based generative models can produce high quality image samples comparable to {GANs}, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32 x 32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. 
To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64 x 64 to 256 x 256. Our score-based models can generate high-fidelity samples that rival best-in-class {GANs} on various image datasets, including {CelebA}, {FFHQ}, and multiple {LSUN} categories.},
	pages = {12438--12448},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Ermon, Stefano},
	urldate = {2023-05-18},
	date = {2020},
	file = {Song_Ermon_2020_Improved Techniques for Training Score-Based Generative Models.pdf:/home/magda/Zotero/storage/RAU6QJHT/Song_Ermon_2020_Improved Techniques for Training Score-Based Generative Models.pdf:application/pdf},
}

@online{escudero_multivariate_2020,
	title = {Multivariate Normal as an Exponential Family Distribution},
	url = {https://maurocamaraescudero.netlify.app/post/multivariate-normal-as-an-exponential-family-distribution/},
	abstract = {How to rewrite a Multivariate Normal distribution as a member of the Exponential Family.},
	titleaddon = {Mauro Camara Escudero},
	author = {Escudero, Mauro Camara},
	urldate = {2023-12-17},
	date = {2020-03-11},
	langid = {english},
	file = {Snapshot:/home/magda/Zotero/storage/4PKSX3ZP/multivariate-normal-as-an-exponential-family-distribution.html:text/html},
}

@online{weng_what_2021,
	title = {What are Diffusion Models?},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	abstract = {[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, {GLIDE}, {unCLIP} and Imagen. [Updated on 2022-08-31: Added latent diffusion model.
So far, I’ve written about three types of generative models, {GAN}, {VAE}, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.},
	author = {Weng, Lilian},
	urldate = {2023-12-17},
	date = {2021-07-11},
	langid = {english},
	note = {Section: posts},
	file = {Snapshot:/home/magda/Zotero/storage/LZ4DFIJA/2021-07-11-diffusion-models.html:text/html},
}



