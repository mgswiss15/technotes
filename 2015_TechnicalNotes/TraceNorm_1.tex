%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MG 10/9/2015 - Trace norm regularization %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Trace norm regularization}\label{sec:TraceNorm}

This uses a lot from \cite{Srebro2004}.

Trace norm (or nuclear norm)\index{Trace norm}\index{Nuclear norm} regularization is often times used as convex relaxation of rank constraint.
For $(m \times n)$ matrix $\vc{X}$ the trace norm is
\begin{equation}
 ||\vc{X}||_* = tr(\sqrt{\vc{XX'}}) = \sum_i^{min\{m,n\}} \sigma_i,
\end{equation}
where $\sigma_i$ are the singular values of $\vc{X} = \vc{\Phi \Sigma \Theta'}$ so that the $p = min\{m,n\}$ singular values $\sigma_i$ are on the diagonal of the $(m \times n)$ matrix $\vc{\Sigma}$,
and $\vc{\Phi} \in \mathbb{R}^{m \times m}$, $\vc{\Theta}  \in \mathbb{R}^{n \times n}$ are unitary so that $\vc{\Phi \Phi'} = \vc{\Phi' \Phi}= \vc{I}_m$ and $\vc{\Theta \Theta'} = \vc{\Theta' \Theta} = \vc{I}_n$.

The trace norm can be seen as the $\ell_1$ norm of the singular values of the matrix.

\begin{small}
\emph{Proof:}
\begin{eqnarray*}
 ||\vc{X}||_* = tr(\sqrt{\vc{XX'}}) = tr(\sqrt{\vc{\Phi \Sigma}^2\vc{\Phi'}}) = tr(\vc{\Phi \Sigma \Phi'}) = tr(\vc{\Sigma}) = \sum_i^{min\{m,n\}} \sigma_i, 
\end{eqnarray*}
where $\vc{XX'} = \vc{\Phi \Sigma}^2\vc{\Phi'}$ is the eigenvalue decomposition.
\end{small}

There is an important lemma, useful for low-rank matrix factorization

\emph{Lemma:} For any matrix $\vc{X} \in \mathbb{R}^{m \times n}$ and $t \in \mathbb{R}$, $||\vc{X}||_* \le t$
iff there exists $\vc{A} \in \mathbb{R}^{m \times m}$ and $\vc{B} \in \mathbb{R}^{n \times n}$ such that
\begin{equation*}
 \begin{bmatrix}
  \vc{A} & \vc{X} \\
  \vc{X'} & \vc{B} \\
 \end{bmatrix}
 \succeq 0 
\qquad \text{ and }
\qquad tr(\vc{A}) + tr(\vc{B}) \le 2t
\end{equation*}

\begin{small}
\emph{Proof:} Every PSD matrix $\vc{Z} \succeq 0$ can be factorised as $\vc{Z} = \vc{\Phi} \vc{\Phi}'$. 
Therefore we can write
\begin{equation}\label{eq:ProofNuclearPSD}
 \begin{bmatrix}
  \vc{A} & \vc{X} \\
  \vc{X'} & \vc{B}
 \end{bmatrix}
 = \vc{\Phi} \vc{\Phi}' = 
 \begin{bmatrix}
  \vc{U} \\ \vc{V}
 \end{bmatrix}
 \begin{bmatrix}
  \vc{U}' & \vc{V}'
 \end{bmatrix} =
 \begin{bmatrix}
  \vc{UU'} & \vc{UV'} \\
  \vc{VU'} & \vc{VV'}
 \end{bmatrix},
\end{equation}
where $\vc{X} = \vc{UV'}$, and $\vc{A} = \vc{UU'}$ and $\vc{B} = \vc{VV'}$.
We now have
\begin{equation}\label{eq:ProofNuclearTrace}
 tr(\vc{A}) + tr(\vc{B}) = tr(\vc{UU'}) + tr(\vc{VV'}) = ||\vc{U}||_F^2 + ||\vc{V}||_F^2 = \sum_i \big( ||\vc{U}_{:i}||_2^2 + ||\vc{V}_{:i}||_2^2 \big) \le 2t
\end{equation}

To find the decomposition $\vc{X} = \vc{UV'}$ we use the singular value decomposition $\vc{X} = \vc{\Phi \Sigma \Theta'}$ with the properties of postmultiplication by diagonal matrix and the outer product approach to matrix multiplication (see Math cheat-sheet)
\begin{equation}\label{eq:ProofNuclearSVD}
 \vc{X} = \vc{\Phi \Sigma \Theta'} = 
\begin{bmatrix}
 \sigma_1 \vc{\Phi}_{:1} & 
 \cdots &
 \sigma_p \vc{\Phi}_{:p}
\end{bmatrix}
\begin{bmatrix}
\vc{\Theta}'_{1:} \\
\vdots \\
\vc{\Theta}'_{p:}
\end{bmatrix} =
\sum_i^p \sigma_i \vc{\Phi}_{:i} \vc{\Theta}_{:i}
= \sum_i^r \sigma_i \vc{\Phi}_{:i} \vc{\Theta}_{:i}
= \sum_i^r \vc{U}_{:i} \vc{V}_{:i} = \vc{U} \vc{V}',
\end{equation}
where $r = rank(\vc{X}) \leq p$ is the number of non-zero singular values and $\vc{U} \in \mathbb{R}^{m \times r}$ with columns $\vc{U}_{:i} = \nu_i \vc{\Phi}_{:i}$, and $\vc{V} \in \mathbb{R}^{n \times r}$ with columns $\vc{V}_{:i} = \eta_i \vc{\Theta}_{:i}$
such that $\nu_i \eta_i = \sigma_i$.

From this we see that the decomposition $\vc{X} = \vc{UV'}$ is not unique since for any $\widetilde{\vc{U}}_{:i} = c_i \nu_i \vc{\Phi}_{:i}$, $\widetilde{\vc{V}}_{:i} = c^{-1}_i \eta_i \vc{\Theta}_{:i}$ we have $\sum_i^r \widetilde{\vc{U}}_{:i} \widetilde{\vc{V}}_{:i} = \sum_i^r c_i \nu_i \vc{\Phi}_{:i} c^{-1}_i \eta_i \vc{\Theta}_{:i} = \sum_i^r \sigma_i \vc{\Phi}_{:i} \vc{\Theta}_{:i} = \vc{X}$.

We now go back and want to check the validity of eq. \eqref{eq:ProofNuclearTrace}.
First, we want to find such decomposition $\vc{X} = \vc{UV'}$ that minimises the the sum of the $\ell_2$ norms in \eqref{eq:ProofNuclearTrace}
\begin{eqnarray*}
 \min_{c} J(c) & := & \sum_i \big( ||\widetilde{\vc{U}}_{:i}||_2^2 + ||\widetilde{\vc{V}}_{:i}||_2^2 \big) = \sum_i^r \big( ||c_i \vc{U}_{:i}||_2^2 + ||c^{-1}_i \vc{V}_{:i}||_2^2 \big) \\ 
& = & \sum_i^r \big( c_i^2 ||\vc{U}_{:i}||_2^2 + ||\vc{V}_{:i}||_2^2/c_i^2 \big)
\end{eqnarray*}
We get for $c_i$ (by taking the derivative and putting equal to zero)
\begin{eqnarray*}
 0 & = & 2c_i ||\vc{U}_{:i}||_2^2 - 2 ||\vc{V}_{:i}||_2^2/c_i^3 \\
c_i^2 & = & \frac{||\vc{U}_{:i}||_2}{||\vc{V}_{:i}||_2}
\end{eqnarray*}
and therefore the minimal 
\begin{eqnarray*}
J(c^*) & = & \sum_i^r \big( ||c_i^* \vc{U}_{:i}||_2^2 + ||c^{*-1}_i \vc{V}_{:i}||_2^2 = \sum_i^r \big( c_i^2 ||\vc{U}_{:i}||_2^2 + ||\vc{V}_{:i}||_2^2/c_i^2 \big) \\
& = & \sum_i^r \big( \frac{||\vc{U}_{:i}||_2}{||\vc{V}_{:i}||_2} ||\vc{U}_{:i}||_2^2 + ||\vc{V}_{:i}||_2^2 \frac{||\vc{V}_{:i}||_2}{||\vc{U}_{:i}||_2} \big) 
= 2 \sum_i^r ||\vc{U}_{:i}||_2 ||\vc{V}_{:i}||_2 \\
& = & 2 \sum_i^r \nu_i \eta_i = 2 \sum_i^r \sigma_i = 2 ||\vc{X}||_* = 2t
\end{eqnarray*}
where we used the unitarity of $\vc{\Phi}$ and $\vc{\Theta}$.
\end{small}

\paragraph{To summarise:} We can decompose a matrix $\vc{X} \in \mathbb{R}^{m \times n}$ of rank $r$ as $\vc{X} = \vc{UV'}$, where $\vc{U} \in \mathbb{R}^{m \times r}$ and $\vc{V} \in \mathbb{R}^{n \times r}$.
However, this decomposition is not unique since for any vector $\vc{c}$ and matrices with columns $\widetilde{\vc{U}}_{:i} = \sum_i c_i \vc{U}_{:i}$ and $\widetilde{\vc{V}}_{:i} = \sum_i c^{-1}_i \vc{V}_{:i}$ we also have $\vc{X} = \widetilde{\vc{U}} \widetilde{\vc{V}}'$.
The minimisation problem with respect to $\vc{U}$ and $\vc{V}$ such that $\vc{U} \vc{V}' = \vc{X}$
\begin{equation}
 \min_{U,V | X = UV} J(\vc{U},\vc{V}) := \frac{1}{2} \big( ||\vc{U}||_F^2 + ||\vc{V}||_F^2 )
\end{equation}
reaches its optimum for matrices with columns $\vc{U}^*_{:i} = \nu_i \vc{\Phi}_{:i}$ and $\vc{V}^*_{:i} = \eta_i \vc{\Theta}_{:i}$, where $\nu_i = \eta_i = \sqrt{\sigma_i}$ coming from the SVD $\vc{X} = \vc{\Phi \Sigma \Theta'}$ and the minimal value is
\begin{equation}
 J(\vc{U}^*,\vc{V}^*) = ||\vc{X}||_*
\end{equation}


\begin{small}
\emph{Proof:}
\begin{eqnarray*}
J(U,V) & = &  \frac{1}{2} \big( ||\vc{U}||_F^2 + ||\vc{V}||_F^2 \big) = \frac{1}{2} \sum_i^r \big( ||\vc{U}_{:i}||_2^2 + ||\vc{V}_{:i}||_2^2 )\\
& = & \frac{1}{2} \sum_i^r \big( ||\nu_i \vc{\Phi}_{:i}||_2^2 + ||\eta_i \vc{\Theta}_{:i}||_2^2 )
= \frac{1}{2} \sum_i^r \big( \nu_i^2  + \eta_i^2) % = \frac{1}{2} \sum_i^r \big( \nu_i^2  + \sigma_i^2/\nu_i^2)
\end{eqnarray*}
\begin{equation}
 \min_{\nu,\eta | \nu_i \eta_i = \sigma_i} \frac{1}{2} \sum_i^r \big( \nu_i^2  + \eta_i^2)
 = \min_{\nu} \frac{1}{2} \sum_i^r \big( \nu_i^2  + \sigma_i^2/\nu_i^2)
\end{equation}
From which we have for the minimising $2\nu_i^{*} - 2\sigma_i^2/\nu_i^{*3} = 0$ and therefore $\nu_i^{*2} = \sigma_i = \eta_i^{*2}$ (from $\nu_i^* = \sqrt{\sigma_i} = \eta_i^*$).
In result the minimum of $J()$ is attained at 
\begin{equation}
 J(\vc{U}^*,\vc{V}^*) = \frac{1}{2} \sum_i^r \big( \nu_i^{*2}  + \eta_i^{*2}) = \frac{1}{2} \sum_i^r \big( \sigma_i  + \sigma_i) = \sum_i^r \sigma_i = ||\vc{X}||_*
\end{equation}

If we impose an additional constraint on $||\vc{V}_{:i}||_2^2 = 1$, we get $\eta_i = 1$ and therefore $\nu_i = \sigma_i$.
The minimum of $J()$ is than attained
\begin{equation}
 J(\vc{U}^*,\vc{V}^*) = \frac{1}{2} \sum_i^r \big( \nu_i^{*2}  + \eta_i^{*2}) = \frac{1}{2} \sum_i^r \big( \sigma_i^2  + 1) = \frac{||\vc{X}||_F^2 + rank(\vc{X})}{2}
\end{equation}
\end{small}

 






\begin{thebibliography}{9}

\bibitem{Srebro2004}
N. Srebro and T. S. Jaakkola, “Maximum-Margin Matrix Factorization,” in NIPS, 2004.

\end{thebibliography}












